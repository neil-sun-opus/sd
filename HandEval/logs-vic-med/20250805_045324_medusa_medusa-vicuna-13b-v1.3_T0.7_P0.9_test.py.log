LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-13b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
HumanEval Tasks:   0%|          | 0/164 [00:00<?, ?it/s]HumanEval Tasks:   1%|          | 1/164 [00:01<05:11,  1.91s/it]HumanEval Tasks:   1%|          | 2/164 [00:02<03:42,  1.37s/it]HumanEval Tasks:   2%|▏         | 3/164 [00:03<02:24,  1.11it/s]HumanEval Tasks:   2%|▏         | 4/164 [00:03<01:57,  1.37it/s]HumanEval Tasks:   3%|▎         | 5/164 [00:04<02:08,  1.24it/s]HumanEval Tasks:   4%|▎         | 6/164 [00:05<02:07,  1.24it/s]HumanEval Tasks:   4%|▍         | 7/164 [00:06<02:41,  1.03s/it]HumanEval Tasks:   5%|▍         | 8/164 [00:07<02:41,  1.03s/it]HumanEval Tasks:   5%|▌         | 9/164 [00:08<02:28,  1.04it/s]HumanEval Tasks:   6%|▌         | 10/164 [00:09<02:27,  1.04it/s]HumanEval Tasks:   7%|▋         | 11/164 [00:12<03:45,  1.47s/it]HumanEval Tasks:   7%|▋         | 12/164 [00:13<03:25,  1.35s/it]HumanEval Tasks:   8%|▊         | 13/164 [00:14<03:25,  1.36s/it]HumanEval Tasks:   9%|▊         | 14/164 [00:15<03:03,  1.22s/it]HumanEval Tasks:   9%|▉         | 15/164 [00:16<02:48,  1.13s/it]HumanEval Tasks:  10%|▉         | 16/164 [00:17<02:12,  1.12it/s]HumanEval Tasks:  10%|█         | 17/164 [00:17<01:55,  1.27it/s]HumanEval Tasks:  11%|█         | 18/164 [00:18<01:54,  1.27it/s]HumanEval Tasks:  12%|█▏        | 19/164 [00:18<01:42,  1.42it/s]HumanEval Tasks:  12%|█▏        | 20/164 [00:19<01:36,  1.50it/s]HumanEval Tasks:  13%|█▎        | 21/164 [00:21<02:57,  1.24s/it]HumanEval Tasks:  13%|█▎        | 22/164 [00:23<03:11,  1.35s/it]HumanEval Tasks:  14%|█▍        | 23/164 [00:24<02:56,  1.25s/it]HumanEval Tasks:  15%|█▍        | 24/164 [00:24<02:10,  1.07it/s]HumanEval Tasks:  15%|█▌        | 25/164 [00:25<01:50,  1.25it/s]HumanEval Tasks:  16%|█▌        | 26/164 [00:26<02:21,  1.03s/it]HumanEval Tasks:  16%|█▋        | 27/164 [00:27<02:09,  1.06it/s]HumanEval Tasks:  17%|█▋        | 28/164 [00:29<02:43,  1.20s/it]HumanEval Tasks:  18%|█▊        | 29/164 [00:29<02:03,  1.09it/s]HumanEval Tasks:  18%|█▊        | 30/164 [00:30<01:59,  1.12it/s]HumanEval Tasks:  19%|█▉        | 31/164 [00:30<01:34,  1.41it/s]HumanEval Tasks:  20%|█▉        | 32/164 [00:32<01:58,  1.11it/s]HumanEval Tasks:  20%|██        | 33/164 [00:34<02:50,  1.30s/it]HumanEval Tasks:  21%|██        | 34/164 [00:35<02:26,  1.13s/it]HumanEval Tasks:  21%|██▏       | 35/164 [00:35<01:58,  1.09it/s]HumanEval Tasks:  22%|██▏       | 36/164 [00:35<01:30,  1.42it/s]HumanEval Tasks:  23%|██▎       | 37/164 [00:36<01:34,  1.35it/s]HumanEval Tasks:  23%|██▎       | 38/164 [00:37<01:29,  1.41it/s]HumanEval Tasks:  24%|██▍       | 39/164 [00:38<01:52,  1.11it/s]HumanEval Tasks:  24%|██▍       | 40/164 [00:40<02:43,  1.32s/it]HumanEval Tasks:  25%|██▌       | 41/164 [00:41<02:25,  1.18s/it]HumanEval Tasks:  26%|██▌       | 42/164 [00:42<02:10,  1.07s/it]HumanEval Tasks:  26%|██▌       | 43/164 [00:42<01:43,  1.17it/s]HumanEval Tasks:  27%|██▋       | 44/164 [00:43<01:30,  1.33it/s]HumanEval Tasks:  27%|██▋       | 45/164 [00:43<01:12,  1.64it/s]HumanEval Tasks:  28%|██▊       | 46/164 [00:43<01:01,  1.91it/s]HumanEval Tasks:  29%|██▊       | 47/164 [00:44<01:01,  1.91it/s]HumanEval Tasks:  29%|██▉       | 48/164 [00:45<01:16,  1.51it/s]HumanEval Tasks:  30%|██▉       | 49/164 [00:45<01:06,  1.72it/s]HumanEval Tasks:  30%|███       | 50/164 [00:46<01:06,  1.71it/s]HumanEval Tasks:  31%|███       | 51/164 [00:47<01:07,  1.68it/s]HumanEval Tasks:  32%|███▏      | 52/164 [00:47<01:03,  1.77it/s]HumanEval Tasks:  32%|███▏      | 53/164 [00:47<00:55,  1.99it/s]HumanEval Tasks:  33%|███▎      | 54/164 [00:48<00:50,  2.19it/s]HumanEval Tasks:  34%|███▎      | 55/164 [00:48<00:53,  2.05it/s]HumanEval Tasks:  34%|███▍      | 56/164 [00:49<00:50,  2.13it/s]HumanEval Tasks:  35%|███▍      | 57/164 [00:51<01:33,  1.14it/s]HumanEval Tasks:  35%|███▌      | 58/164 [00:52<01:52,  1.06s/it]HumanEval Tasks:  36%|███▌      | 59/164 [00:52<01:30,  1.16it/s]HumanEval Tasks:  37%|███▋      | 60/164 [00:53<01:22,  1.26it/s]HumanEval Tasks:  37%|███▋      | 61/164 [00:53<01:06,  1.55it/s]HumanEval Tasks:  38%|███▊      | 62/164 [00:55<01:26,  1.19it/s]HumanEval Tasks:  38%|███▊      | 63/164 [00:55<01:10,  1.42it/s]HumanEval Tasks:  39%|███▉      | 64/164 [00:56<01:05,  1.53it/s]HumanEval Tasks:  40%|███▉      | 65/164 [00:56<01:10,  1.41it/s]HumanEval Tasks:  40%|████      | 66/164 [00:58<01:24,  1.16it/s]HumanEval Tasks:  41%|████      | 67/164 [00:58<01:11,  1.36it/s]HumanEval Tasks:  41%|████▏     | 68/164 [00:59<01:12,  1.32it/s]HumanEval Tasks:  42%|████▏     | 69/164 [01:00<01:08,  1.39it/s]HumanEval Tasks:  43%|████▎     | 70/164 [01:00<01:08,  1.37it/s]HumanEval Tasks:  43%|████▎     | 71/164 [01:01<01:08,  1.35it/s]HumanEval Tasks:  44%|████▍     | 72/164 [01:02<01:05,  1.41it/s]HumanEval Tasks:  45%|████▍     | 73/164 [01:02<00:56,  1.62it/s]HumanEval Tasks:  45%|████▌     | 74/164 [01:03<00:58,  1.54it/s]HumanEval Tasks:  46%|████▌     | 75/164 [01:03<00:56,  1.58it/s]HumanEval Tasks:  46%|████▋     | 76/164 [01:04<01:00,  1.45it/s]HumanEval Tasks:  47%|████▋     | 77/164 [01:04<00:47,  1.83it/s]HumanEval Tasks:  48%|████▊     | 78/164 [01:05<00:39,  2.20it/s]HumanEval Tasks:  48%|████▊     | 79/164 [01:05<00:41,  2.03it/s]HumanEval Tasks:  49%|████▉     | 80/164 [01:06<00:46,  1.80it/s]HumanEval Tasks:  49%|████▉     | 81/164 [01:07<00:54,  1.52it/s]HumanEval Tasks:  50%|█████     | 82/164 [01:09<01:33,  1.13s/it]HumanEval Tasks:  51%|█████     | 83/164 [01:10<01:21,  1.01s/it]HumanEval Tasks:  51%|█████     | 84/164 [01:11<01:15,  1.06it/s]HumanEval Tasks:  52%|█████▏    | 85/164 [01:11<01:04,  1.23it/s]HumanEval Tasks:  52%|█████▏    | 86/164 [01:12<01:05,  1.18it/s]HumanEval Tasks:  53%|█████▎    | 87/164 [01:13<01:11,  1.07it/s]HumanEval Tasks:  54%|█████▎    | 88/164 [01:14<01:16,  1.01s/it]HumanEval Tasks:  54%|█████▍    | 89/164 [01:16<01:20,  1.07s/it]HumanEval Tasks:  55%|█████▍    | 90/164 [01:16<01:08,  1.07it/s]HumanEval Tasks:  55%|█████▌    | 91/164 [01:16<00:52,  1.38it/s]HumanEval Tasks:  56%|█████▌    | 92/164 [01:17<00:54,  1.32it/s]HumanEval Tasks:  57%|█████▋    | 93/164 [01:18<00:46,  1.52it/s]HumanEval Tasks:  57%|█████▋    | 94/164 [01:19<00:51,  1.35it/s]HumanEval Tasks:  58%|█████▊    | 95/164 [01:20<00:55,  1.24it/s]HumanEval Tasks:  59%|█████▊    | 96/164 [01:23<01:44,  1.53s/it]HumanEval Tasks:  59%|█████▉    | 97/164 [01:23<01:21,  1.22s/it]HumanEval Tasks:  60%|█████▉    | 98/164 [01:24<01:00,  1.09it/s]HumanEval Tasks:  60%|██████    | 99/164 [01:24<00:50,  1.28it/s]HumanEval Tasks:  61%|██████    | 100/164 [01:25<00:55,  1.15it/s]HumanEval Tasks:  62%|██████▏   | 101/164 [01:26<00:55,  1.13it/s]HumanEval Tasks:  62%|██████▏   | 102/164 [01:26<00:41,  1.48it/s]HumanEval Tasks:  63%|██████▎   | 103/164 [01:27<00:43,  1.41it/s]HumanEval Tasks:  63%|██████▎   | 104/164 [01:28<00:43,  1.37it/s]HumanEval Tasks:  64%|██████▍   | 105/164 [01:28<00:36,  1.60it/s]HumanEval Tasks:  65%|██████▍   | 106/164 [01:29<00:38,  1.50it/s]HumanEval Tasks:  65%|██████▌   | 107/164 [01:29<00:34,  1.63it/s]HumanEval Tasks:  66%|██████▌   | 108/164 [01:30<00:33,  1.66it/s]HumanEval Tasks:  66%|██████▋   | 109/164 [01:31<00:35,  1.56it/s]HumanEval Tasks:  67%|██████▋   | 110/164 [01:35<01:26,  1.60s/it]HumanEval Tasks:  68%|██████▊   | 111/164 [01:35<01:02,  1.19s/it]HumanEval Tasks:  68%|██████▊   | 112/164 [01:35<00:52,  1.01s/it]HumanEval Tasks:  69%|██████▉   | 113/164 [01:36<00:49,  1.03it/s]HumanEval Tasks:  70%|██████▉   | 114/164 [01:37<00:46,  1.08it/s]HumanEval Tasks:  70%|███████   | 115/164 [01:38<00:40,  1.20it/s]HumanEval Tasks:  71%|███████   | 116/164 [01:39<00:47,  1.00it/s]HumanEval Tasks:  71%|███████▏  | 117/164 [01:39<00:35,  1.31it/s]HumanEval Tasks:  72%|███████▏  | 118/164 [01:40<00:31,  1.46it/s]HumanEval Tasks:  73%|███████▎  | 119/164 [01:41<00:37,  1.19it/s]HumanEval Tasks:  73%|███████▎  | 120/164 [01:42<00:41,  1.07it/s]HumanEval Tasks:  74%|███████▍  | 121/164 [01:43<00:35,  1.22it/s]HumanEval Tasks:  74%|███████▍  | 122/164 [01:43<00:30,  1.36it/s]HumanEval Tasks:  75%|███████▌  | 123/164 [01:44<00:29,  1.39it/s]HumanEval Tasks:  76%|███████▌  | 124/164 [01:47<01:00,  1.52s/it]HumanEval Tasks:  76%|███████▌  | 125/164 [01:49<01:06,  1.70s/it]HumanEval Tasks:  77%|███████▋  | 126/164 [01:51<00:58,  1.53s/it]HumanEval Tasks:  77%|███████▋  | 127/164 [01:51<00:43,  1.19s/it]HumanEval Tasks:  78%|███████▊  | 128/164 [01:52<00:41,  1.15s/it]HumanEval Tasks:  79%|███████▊  | 129/164 [01:53<00:34,  1.02it/s]HumanEval Tasks:  79%|███████▉  | 130/164 [01:54<00:38,  1.12s/it]HumanEval Tasks:  80%|███████▉  | 131/164 [01:57<00:54,  1.64s/it]HumanEval Tasks:  80%|████████  | 132/164 [01:57<00:40,  1.27s/it]HumanEval Tasks:  81%|████████  | 133/164 [01:58<00:34,  1.12s/it]HumanEval Tasks:  82%|████████▏ | 134/164 [01:58<00:25,  1.16it/s]HumanEval Tasks:  82%|████████▏ | 135/164 [01:59<00:23,  1.22it/s]HumanEval Tasks:  83%|████████▎ | 136/164 [02:00<00:20,  1.38it/s]HumanEval Tasks:  84%|████████▎ | 137/164 [02:00<00:17,  1.54it/s]HumanEval Tasks:  84%|████████▍ | 138/164 [02:02<00:26,  1.01s/it]HumanEval Tasks:  85%|████████▍ | 139/164 [02:03<00:29,  1.16s/it]HumanEval Tasks:  85%|████████▌ | 140/164 [02:05<00:28,  1.19s/it]HumanEval Tasks:  86%|████████▌ | 141/164 [02:05<00:21,  1.06it/s]HumanEval Tasks:  87%|████████▋ | 142/164 [02:07<00:24,  1.11s/it]HumanEval Tasks:  87%|████████▋ | 143/164 [02:07<00:19,  1.05it/s]HumanEval Tasks:  88%|████████▊ | 144/164 [02:08<00:16,  1.18it/s]HumanEval Tasks:  88%|████████▊ | 145/164 [02:09<00:17,  1.08it/s]HumanEval Tasks:  89%|████████▉ | 146/164 [02:10<00:16,  1.07it/s]HumanEval Tasks:  90%|████████▉ | 147/164 [02:10<00:13,  1.22it/s]HumanEval Tasks:  90%|█████████ | 148/164 [02:11<00:13,  1.19it/s]HumanEval Tasks:  91%|█████████ | 149/164 [02:12<00:14,  1.05it/s]HumanEval Tasks:  91%|█████████▏| 150/164 [02:13<00:10,  1.34it/s]HumanEval Tasks:  92%|█████████▏| 151/164 [02:13<00:09,  1.39it/s]HumanEval Tasks:  93%|█████████▎| 152/164 [02:14<00:07,  1.64it/s]HumanEval Tasks:  93%|█████████▎| 153/164 [02:14<00:05,  1.83it/s]HumanEval Tasks:  94%|█████████▍| 154/164 [02:18<00:15,  1.59s/it]HumanEval Tasks:  95%|█████████▍| 155/164 [02:19<00:11,  1.33s/it]HumanEval Tasks:  95%|█████████▌| 156/164 [02:19<00:09,  1.13s/it]HumanEval Tasks:  96%|█████████▌| 157/164 [02:21<00:08,  1.20s/it]HumanEval Tasks:  96%|█████████▋| 158/164 [02:22<00:07,  1.33s/it]HumanEval Tasks:  97%|█████████▋| 159/164 [02:23<00:05,  1.11s/it]HumanEval Tasks:  98%|█████████▊| 160/164 [02:24<00:03,  1.09it/s]HumanEval Tasks:  98%|█████████▊| 161/164 [02:26<00:04,  1.35s/it]HumanEval Tasks:  99%|█████████▉| 162/164 [02:27<00:02,  1.15s/it]HumanEval Tasks:  99%|█████████▉| 163/164 [02:28<00:01,  1.24s/it]HumanEval Tasks: 100%|██████████| 164/164 [02:28<00:00,  1.01it/s]HumanEval Tasks: 100%|██████████| 164/164 [02:28<00:00,  1.10it/s]
Average accept_length: 2.1666667461395264
Average accept_length: 1.731707215309143
Average accept_length: 2.230769395828247
Average accept_length: 2.21052622795105
Average accept_length: 2.1282050609588623
Average accept_length: 1.8181818723678589
Average accept_length: 1.2698413133621216
Average accept_length: 1.4883720874786377
Average accept_length: 2.060606002807617
Average accept_length: 2.325000047683716
Average accept_length: 1.8108108043670654
Average accept_length: 1.4666666984558105
Average accept_length: 1.7931034564971924
Average accept_length: 1.736842155456543
Average accept_length: 2.0512821674346924
Average accept_length: 1.7142858505249023
Average accept_length: 2.6363637447357178
Average accept_length: 2.34375
Average accept_length: 2.190476179122925
Average accept_length: 1.875
Average accept_length: 1.935185194015503
Average accept_length: 1.776119351387024
Average accept_length: 1.883720874786377
Average accept_length: 2.4285714626312256
Average accept_length: 1.649999976158142
Average accept_length: 1.6666667461395264
Average accept_length: 2.2580645084381104
Average accept_length: 1.5131579637527466
Average accept_length: 2.4000000953674316
Average accept_length: 1.971428632736206
Average accept_length: 2.090909242630005
Average accept_length: 2.087719202041626
Average accept_length: 1.4731183052062988
Average accept_length: 1.8000000715255737
Average accept_length: 2.470588207244873
Average accept_length: 1.75
Average accept_length: 2.171428680419922
Average accept_length: 1.576923131942749
Average accept_length: 1.3750001192092896
Average accept_length: 1.7857142686843872
Average accept_length: 1.971428632736206
Average accept_length: 1.7352941036224365
Average accept_length: 1.4285714626312256
Average accept_length: 2.190476179122925
Average accept_length: 1.6363637447357178
Average accept_length: 2.0
Average accept_length: 1.9047619104385376
Average accept_length: 2.121951103210449
Average accept_length: 1.875
Average accept_length: 1.2916667461395264
Average accept_length: 1.2799999713897705
Average accept_length: 1.7000000476837158
Average accept_length: 1.571428656578064
Average accept_length: 2.142857313156128
Average accept_length: 1.5909091234207153
Average accept_length: 2.117647171020508
Average accept_length: 1.3896104097366333
Average accept_length: 1.7619048357009888
Average accept_length: 1.5
Average accept_length: 1.461538553237915
Average accept_length: 2.090909242630005
Average accept_length: 1.4727271795272827
Average accept_length: 1.2666667699813843
Average accept_length: 2.045454502105713
Average accept_length: 1.6857142448425293
Average accept_length: 1.5576924085617065
Average accept_length: 1.9444444179534912
Average accept_length: 1.7272727489471436
Average accept_length: 1.959999918937683
Average accept_length: 1.7096773386001587
Average accept_length: 1.875
Average accept_length: 1.7307692766189575
Average accept_length: 1.5625
Average accept_length: 1.9000000953674316
Average accept_length: 1.6666667461395264
Average accept_length: 1.8823529481887817
Average accept_length: 2.125
Average accept_length: 1.6666666269302368
Average accept_length: 2.04347825050354
Average accept_length: 1.5862069129943848
Average accept_length: 1.7837837934494019
Average accept_length: 1.8297871351242065
Average accept_length: 2.1000001430511475
Average accept_length: 1.65625
Average accept_length: 1.8095238208770752
Average accept_length: 1.8205128908157349
Average accept_length: 1.6666667461395264
Average accept_length: 1.6999999284744263
Average accept_length: 1.7450981140136719
Average accept_length: 2.0
Average accept_length: 2.444444417953491
Average accept_length: 1.6857142448425293
Average accept_length: 2.1764705181121826
Average accept_length: 1.6842105388641357
Average accept_length: 1.8205128908157349
Average accept_length: 1.3629629611968994
Average accept_length: 2.3500001430511475
Average accept_length: 1.875
Average accept_length: 2.1052632331848145
Average accept_length: 1.6888889074325562
Average accept_length: 1.769230842590332
Average accept_length: 2.0
Average accept_length: 2.1818182468414307
Average accept_length: 1.8125
Average accept_length: 1.533333420753479
Average accept_length: 1.3548386096954346
Average accept_length: 2.299999952316284
Average accept_length: 2.3478262424468994
Average accept_length: 2.0333335399627686
Average accept_length: 1.556249976158142
Average accept_length: 2.125
Average accept_length: 2.0
Average accept_length: 1.75
Average accept_length: 2.029411792755127
Average accept_length: 2.1599998474121094
Average accept_length: 1.894736886024475
Average accept_length: 1.875
Average accept_length: 1.9500000476837158
Average accept_length: 1.6470588445663452
Average accept_length: 1.7916667461395264
Average accept_length: 1.772727370262146
Average accept_length: 2.1363637447357178
Average accept_length: 1.821428656578064
Average accept_length: 1.4366196393966675
Average accept_length: 2.2298851013183594
Average accept_length: 1.6382977962493896
Average accept_length: 2.266666889190674
Average accept_length: 1.772727370262146
Average accept_length: 2.04347825050354
Average accept_length: 1.8666667938232422
Average accept_length: 1.7416667938232422
Average accept_length: 1.8125
Average accept_length: 1.78125
Average accept_length: 1.7000000476837158
Average accept_length: 1.5862069129943848
Average accept_length: 2.049999952316284
Average accept_length: 1.8421052694320679
Average accept_length: 1.5324674844741821
Average accept_length: 1.61904776096344
Average accept_length: 1.7307692766189575
Average accept_length: 2.0
Average accept_length: 2.0
Average accept_length: 2.2608695030212402
Average accept_length: 1.9583333730697632
Average accept_length: 1.6739131212234497
Average accept_length: 1.899999976158142
Average accept_length: 2.17391300201416
Average accept_length: 2.222222328186035
Average accept_length: 1.6078431606292725
Average accept_length: 2.0
Average accept_length: 1.9629629850387573
Average accept_length: 1.4285714626312256
Average accept_length: 1.4666666984558105
Average accept_length: 1.3727810382843018
Average accept_length: 1.862069010734558
Average accept_length: 2.25
Average accept_length: 1.637930989265442
Average accept_length: 1.9253730773925781
Average accept_length: 2.125
Average accept_length: 1.4444444179534912
Average accept_length: 1.505050539970398
Average accept_length: 2.107142925262451
Average accept_length: 1.5081965923309326
Average accept_length: 2.5625
🔄 Completions saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-13b-v1.3/humaneval_samples.jsonl

✅  Results saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-13b-v1.3/humaneval_metrics.json
{
  "num_samples": 164,
  "samples_file": "outputs/medusa-vicuna-13b-v1.3/humaneval_samples.jsonl"
}
