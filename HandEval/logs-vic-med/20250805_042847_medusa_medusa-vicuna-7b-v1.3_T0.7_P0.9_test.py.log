LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.09s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-7b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
HumanEval Tasks:   0%|          | 0/164 [00:00<?, ?it/s]HumanEval Tasks:   1%|          | 1/164 [00:01<04:25,  1.63s/it]HumanEval Tasks:   1%|          | 2/164 [00:02<02:28,  1.09it/s]HumanEval Tasks:   2%|â–         | 3/164 [00:02<01:58,  1.35it/s]HumanEval Tasks:   2%|â–         | 4/164 [00:02<01:37,  1.64it/s]HumanEval Tasks:   3%|â–Ž         | 5/164 [00:04<02:11,  1.21it/s]HumanEval Tasks:   4%|â–Ž         | 6/164 [00:05<02:31,  1.04it/s]HumanEval Tasks:   4%|â–         | 7/164 [00:06<02:48,  1.08s/it]HumanEval Tasks:   5%|â–         | 8/164 [00:07<02:43,  1.05s/it]HumanEval Tasks:   5%|â–Œ         | 9/164 [00:08<02:29,  1.04it/s]HumanEval Tasks:   6%|â–Œ         | 10/164 [00:09<02:41,  1.05s/it]HumanEval Tasks:   7%|â–‹         | 11/164 [00:10<02:23,  1.06it/s]HumanEval Tasks:   7%|â–‹         | 12/164 [00:11<02:17,  1.10it/s]HumanEval Tasks:   8%|â–Š         | 13/164 [00:11<01:58,  1.27it/s]HumanEval Tasks:   9%|â–Š         | 14/164 [00:12<02:09,  1.16it/s]HumanEval Tasks:   9%|â–‰         | 15/164 [00:13<01:58,  1.26it/s]HumanEval Tasks:  10%|â–‰         | 16/164 [00:13<01:33,  1.59it/s]HumanEval Tasks:  10%|â–ˆ         | 17/164 [00:14<01:32,  1.58it/s]HumanEval Tasks:  11%|â–ˆ         | 18/164 [00:14<01:32,  1.58it/s]HumanEval Tasks:  12%|â–ˆâ–        | 19/164 [00:15<01:20,  1.81it/s]HumanEval Tasks:  12%|â–ˆâ–        | 20/164 [00:15<01:10,  2.04it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 21/164 [00:17<02:13,  1.07it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 22/164 [00:19<02:41,  1.13s/it]HumanEval Tasks:  14%|â–ˆâ–        | 23/164 [00:19<02:07,  1.10it/s]HumanEval Tasks:  15%|â–ˆâ–        | 24/164 [00:19<01:35,  1.46it/s]HumanEval Tasks:  15%|â–ˆâ–Œ        | 25/164 [00:20<01:28,  1.57it/s]HumanEval Tasks:  16%|â–ˆâ–Œ        | 26/164 [00:20<01:21,  1.70it/s]HumanEval Tasks:  16%|â–ˆâ–‹        | 27/164 [00:21<01:16,  1.79it/s]HumanEval Tasks:  17%|â–ˆâ–‹        | 28/164 [00:21<01:04,  2.11it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 29/164 [00:21<00:52,  2.56it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 30/164 [00:22<00:49,  2.69it/s]HumanEval Tasks:  19%|â–ˆâ–‰        | 31/164 [00:22<00:43,  3.04it/s]HumanEval Tasks:  20%|â–ˆâ–‰        | 32/164 [00:23<01:16,  1.73it/s]HumanEval Tasks:  20%|â–ˆâ–ˆ        | 33/164 [00:25<01:56,  1.12it/s]HumanEval Tasks:  21%|â–ˆâ–ˆ        | 34/164 [00:26<01:58,  1.10it/s]HumanEval Tasks:  21%|â–ˆâ–ˆâ–       | 35/164 [00:26<01:29,  1.44it/s]HumanEval Tasks:  22%|â–ˆâ–ˆâ–       | 36/164 [00:26<01:10,  1.82it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 37/164 [00:26<01:00,  2.09it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 38/164 [00:27<00:56,  2.25it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 39/164 [00:27<00:50,  2.47it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 40/164 [00:28<01:16,  1.61it/s]HumanEval Tasks:  25%|â–ˆâ–ˆâ–Œ       | 41/164 [00:29<01:22,  1.49it/s]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 42/164 [00:29<01:16,  1.60it/s]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 43/164 [00:30<01:01,  1.95it/s]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 44/164 [00:30<01:00,  1.99it/s]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 45/164 [00:31<01:02,  1.90it/s]HumanEval Tasks:  28%|â–ˆâ–ˆâ–Š       | 46/164 [00:31<00:50,  2.32it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–Š       | 47/164 [00:32<01:13,  1.58it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–‰       | 48/164 [00:33<01:15,  1.54it/s]HumanEval Tasks:  30%|â–ˆâ–ˆâ–‰       | 49/164 [00:33<01:20,  1.44it/s]HumanEval Tasks:  30%|â–ˆâ–ˆâ–ˆ       | 50/164 [00:34<01:24,  1.34it/s]HumanEval Tasks:  31%|â–ˆâ–ˆâ–ˆ       | 51/164 [00:35<01:15,  1.49it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/164 [00:35<01:06,  1.69it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 53/164 [00:36<00:55,  2.00it/s]HumanEval Tasks:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/164 [00:36<00:43,  2.53it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 55/164 [00:36<00:55,  1.97it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–      | 56/164 [00:37<00:46,  2.30it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–      | 57/164 [00:37<00:44,  2.38it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/164 [00:38<00:59,  1.79it/s]HumanEval Tasks:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 59/164 [00:38<00:50,  2.07it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/164 [00:39<00:49,  2.12it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 61/164 [00:39<00:40,  2.54it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/164 [00:40<00:56,  1.81it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 63/164 [00:40<00:48,  2.08it/s]HumanEval Tasks:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 64/164 [00:41<00:54,  1.84it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 65/164 [00:41<00:50,  1.98it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/164 [00:42<00:42,  2.30it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 67/164 [00:42<00:39,  2.48it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/164 [00:42<00:41,  2.33it/s]HumanEval Tasks:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 69/164 [00:43<00:41,  2.29it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/164 [00:43<00:45,  2.06it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 71/164 [00:44<00:37,  2.51it/s]HumanEval Tasks:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/164 [00:44<00:48,  1.91it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/164 [00:45<00:47,  1.93it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/164 [00:46<00:48,  1.84it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 75/164 [00:46<00:46,  1.92it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/164 [00:47<00:49,  1.78it/s]HumanEval Tasks:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 77/164 [00:47<00:40,  2.13it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/164 [00:47<00:39,  2.18it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 79/164 [00:50<01:45,  1.24s/it]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/164 [00:51<01:31,  1.09s/it]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 81/164 [00:53<01:41,  1.22s/it]HumanEval Tasks:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/164 [00:53<01:22,  1.01s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/164 [00:54<01:09,  1.17it/s]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 84/164 [00:54<01:00,  1.33it/s]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/164 [00:55<00:55,  1.42it/s]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 86/164 [00:55<00:52,  1.47it/s]HumanEval Tasks:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/164 [00:56<00:47,  1.62it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 88/164 [00:56<00:39,  1.92it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/164 [00:56<00:31,  2.40it/s]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/164 [00:57<00:28,  2.56it/s]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/164 [00:57<00:25,  2.92it/s]HumanEval Tasks:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 92/164 [00:57<00:27,  2.59it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/164 [00:58<00:29,  2.44it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 94/164 [00:59<00:35,  1.95it/s]HumanEval Tasks:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/164 [01:02<01:34,  1.38s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 96/164 [01:02<01:11,  1.06s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/164 [01:03<01:02,  1.07it/s]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 98/164 [01:03<00:47,  1.38it/s]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/164 [01:04<00:41,  1.58it/s]HumanEval Tasks:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 100/164 [01:04<00:31,  2.01it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/164 [01:04<00:33,  1.89it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 102/164 [01:05<00:26,  2.30it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/164 [01:05<00:24,  2.52it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 104/164 [01:06<00:42,  1.40it/s]HumanEval Tasks:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/164 [01:07<00:35,  1.67it/s]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/164 [01:07<00:35,  1.63it/s]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/164 [01:08<00:32,  1.78it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 108/164 [01:08<00:31,  1.79it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/164 [01:09<00:26,  2.08it/s]HumanEval Tasks:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 110/164 [01:09<00:24,  2.17it/s]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/164 [01:10<00:37,  1.40it/s]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 112/164 [01:11<00:32,  1.61it/s]HumanEval Tasks:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/164 [01:11<00:31,  1.63it/s]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 114/164 [01:12<00:28,  1.78it/s]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/164 [01:12<00:26,  1.83it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 116/164 [01:13<00:29,  1.65it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/164 [01:13<00:25,  1.84it/s]HumanEval Tasks:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 118/164 [01:14<00:23,  1.99it/s]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/164 [01:15<00:31,  1.41it/s]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 120/164 [01:16<00:36,  1.22it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/164 [01:17<00:30,  1.41it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/164 [01:17<00:24,  1.74it/s]HumanEval Tasks:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/164 [01:17<00:22,  1.85it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 124/164 [01:18<00:20,  1.97it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 125/164 [01:22<01:03,  1.62s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 126/164 [01:22<00:46,  1.22s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 127/164 [01:22<00:33,  1.09it/s]HumanEval Tasks:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 128/164 [01:24<00:37,  1.03s/it]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 129/164 [01:24<00:31,  1.12it/s]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 130/164 [01:26<00:33,  1.00it/s]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 131/164 [01:26<00:29,  1.10it/s]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 132/164 [01:27<00:24,  1.30it/s]HumanEval Tasks:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 133/164 [01:28<00:24,  1.25it/s]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 134/164 [01:28<00:18,  1.59it/s]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 135/164 [01:28<00:17,  1.66it/s]HumanEval Tasks:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 136/164 [01:29<00:14,  1.88it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 137/164 [01:29<00:15,  1.71it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 138/164 [01:31<00:19,  1.31it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139/164 [01:31<00:17,  1.46it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 140/164 [01:31<00:13,  1.79it/s]HumanEval Tasks:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 141/164 [01:32<00:11,  2.05it/s]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 142/164 [01:33<00:17,  1.24it/s]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 143/164 [01:34<00:14,  1.41it/s]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 144/164 [01:39<00:39,  2.00s/it]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 145/164 [01:39<00:29,  1.57s/it]HumanEval Tasks:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/164 [01:40<00:21,  1.20s/it]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 147/164 [01:41<00:19,  1.16s/it]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 148/164 [01:42<00:16,  1.05s/it]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 149/164 [01:44<00:22,  1.50s/it]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 150/164 [01:44<00:15,  1.13s/it]HumanEval Tasks:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 151/164 [01:45<00:12,  1.02it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 152/164 [01:45<00:09,  1.29it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 153/164 [01:46<00:07,  1.46it/s]HumanEval Tasks:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 154/164 [01:46<00:06,  1.66it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 155/164 [01:47<00:05,  1.74it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 156/164 [01:47<00:04,  1.92it/s]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 157/164 [01:49<00:05,  1.17it/s]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 158/164 [01:49<00:04,  1.46it/s]HumanEval Tasks:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 159/164 [01:49<00:02,  1.81it/s]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 160/164 [01:50<00:02,  1.91it/s]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 161/164 [01:51<00:02,  1.32it/s]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 162/164 [01:51<00:01,  1.52it/s]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 163/164 [01:52<00:00,  1.81it/s]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [01:52<00:00,  1.96it/s]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [01:52<00:00,  1.46it/s]
Average accept_length: 2.0
Average accept_length: 0.8095238208770752
Average accept_length: 1.5384615659713745
Average accept_length: 1.8000000715255737
Average accept_length: 1.6885244846343994
Average accept_length: 1.7704917192459106
Average accept_length: 1.492537260055542
Average accept_length: 1.4399999380111694
Average accept_length: 1.894736886024475
Average accept_length: 1.9047620296478271
Average accept_length: 1.9117647409439087
Average accept_length: 1.6190476417541504
Average accept_length: 2.0799999237060547
Average accept_length: 1.396226406097412
Average accept_length: 1.71875
Average accept_length: 2.0
Average accept_length: 2.0
Average accept_length: 1.6875
Average accept_length: 2.0
Average accept_length: 1.1764706373214722
Average accept_length: 1.8499999046325684
Average accept_length: 1.7560975551605225
Average accept_length: 1.7777777910232544
Average accept_length: 2.0
Average accept_length: 1.884615421295166
Average accept_length: 1.7083333730697632
Average accept_length: 2.0799999237060547
Average accept_length: 1.9230769872665405
Average accept_length: 2.1111111640930176
Average accept_length: 1.8125
Average accept_length: 2.090909242630005
Average accept_length: 2.01694917678833
Average accept_length: 1.6024096012115479
Average accept_length: 1.6530611515045166
Average accept_length: 2.0
Average accept_length: 1.5
Average accept_length: 1.8000000715255737
Average accept_length: 1.5555555820465088
Average accept_length: 0.8000000715255737
Average accept_length: 1.824561357498169
Average accept_length: 1.8000000715255737
Average accept_length: 1.9230769872665405
Average accept_length: 1.6666667461395264
Average accept_length: 2.4166667461395264
Average accept_length: 2.068965435028076
Average accept_length: 1.600000023841858
Average accept_length: 1.8571429252624512
Average accept_length: 1.7999999523162842
Average accept_length: 1.600000023841858
Average accept_length: 1.8372093439102173
Average accept_length: 0.7199999690055847
Average accept_length: 1.399999976158142
Average accept_length: 1.571428656578064
Average accept_length: 2.2857143878936768
Average accept_length: 1.8974359035491943
Average accept_length: 2.461538553237915
Average accept_length: 1.6842105388641357
Average accept_length: 1.7777777910232544
Average accept_length: 1.6000001430511475
Average accept_length: 2.1363637447357178
Average accept_length: 2.0
Average accept_length: 1.2553191184997559
Average accept_length: 0.8666667342185974
Average accept_length: 2.057142972946167
Average accept_length: 1.7619048357009888
Average accept_length: 1.2307692766189575
Average accept_length: 1.5625
Average accept_length: 1.4166667461395264
Average accept_length: 1.5454546213150024
Average accept_length: 1.8666667938232422
Average accept_length: 1.888888955116272
Average accept_length: 1.5750000476837158
Average accept_length: 1.4399999380111694
Average accept_length: 2.066666841506958
Average accept_length: 1.7826087474822998
Average accept_length: 1.875
Average accept_length: 2.4166667461395264
Average accept_length: 2.047619104385376
Average accept_length: 1.5870966911315918
Average accept_length: 1.5405405759811401
Average accept_length: 1.8051948547363281
Average accept_length: 1.653846263885498
Average accept_length: 1.959999918937683
Average accept_length: 1.5999999046325684
Average accept_length: 1.633333444595337
Average accept_length: 1.6774193048477173
Average accept_length: 2.0833334922790527
Average accept_length: 1.6428571939468384
Average accept_length: 1.5
Average accept_length: 1.5
Average accept_length: 2.0
Average accept_length: 1.625
Average accept_length: 1.47826087474823
Average accept_length: 1.6315789222717285
Average accept_length: 2.0289015769958496
Average accept_length: 1.533333420753479
Average accept_length: 1.7575757503509521
Average accept_length: 1.454545497894287
Average accept_length: 2.049999952316284
Average accept_length: 1.5
Average accept_length: 1.8666667938232422
Average accept_length: 2.0
Average accept_length: 1.8666667938232422
Average accept_length: 1.6399999856948853
Average accept_length: 1.4375
Average accept_length: 1.59375
Average accept_length: 2.2857143878936768
Average accept_length: 2.2962963581085205
Average accept_length: 2.4285714626312256
Average accept_length: 2.25
Average accept_length: 1.8181818723678589
Average accept_length: 2.1000001430511475
Average accept_length: 1.2666667699813843
Average accept_length: 1.6363637447357178
Average accept_length: 2.119999885559082
Average accept_length: 1.9729729890823364
Average accept_length: 1.736842155456543
Average accept_length: 1.399999976158142
Average accept_length: 1.4918031692504883
Average accept_length: 1.1818181276321411
Average accept_length: 1.772727370262146
Average accept_length: 1.9230769872665405
Average accept_length: 2.04347825050354
Average accept_length: 2.047619104385376
Average accept_length: 1.5879629850387573
Average accept_length: 1.8666667938232422
Average accept_length: 2.0
Average accept_length: 1.8615384101867676
Average accept_length: 1.655172348022461
Average accept_length: 1.9047620296478271
Average accept_length: 1.8571429252624512
Average accept_length: 2.409090995788574
Average accept_length: 1.5555555820465088
Average accept_length: 1.8181818723678589
Average accept_length: 1.6666666269302368
Average accept_length: 2.1111111640930176
Average accept_length: 1.8333333730697632
Average accept_length: 1.475409746170044
Average accept_length: 2.119999885559082
Average accept_length: 1.846153974533081
Average accept_length: 1.75
Average accept_length: 1.835443139076233
Average accept_length: 1.9583333730697632
Average accept_length: 1.890625
Average accept_length: 1.7586207389831543
Average accept_length: 1.625
Average accept_length: 1.7777777910232544
Average accept_length: 1.7804877758026123
Average accept_length: 1.305343508720398
Average accept_length: 1.9166667461395264
Average accept_length: 1.9393939971923828
Average accept_length: 1.6428571939468384
Average accept_length: 1.9166667461395264
Average accept_length: 1.649999976158142
Average accept_length: 1.769230842590332
Average accept_length: 1.149999976158142
Average accept_length: 1.4047619104385376
Average accept_length: 1.3571429252624512
Average accept_length: 1.75
Average accept_length: 0.8636363744735718
Average accept_length: 1.5
Average accept_length: 1.3333333730697632
Average accept_length: 1.8000000715255737
Average accept_length: 1.75
ðŸ”„ Completions saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-7b-v1.3/humaneval_samples.jsonl

âœ…  Results saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-7b-v1.3/humaneval_metrics.json
{
  "num_samples": 164,
  "samples_file": "outputs/medusa-vicuna-7b-v1.3/humaneval_samples.jsonl"
}
