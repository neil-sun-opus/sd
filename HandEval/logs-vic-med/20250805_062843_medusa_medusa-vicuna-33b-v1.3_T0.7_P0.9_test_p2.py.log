LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.50s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:07,  1.50s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.54s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.53s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:03,  1.52s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.43s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-33b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 0/200 [00:00<?, ?it/s]Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 1/200 [00:11<37:56, 11.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:   1%|          | 2/200 [00:22<36:41, 11.12s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 3/200 [00:32<34:21, 10.47s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 4/200 [00:38<29:28,  9.03s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▎         | 5/200 [00:43<24:43,  7.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:   3%|▎         | 6/200 [00:55<28:58,  8.96s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▎         | 7/200 [01:04<28:33,  8.88s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 8/200 [01:05<20:53,  6.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 9/200 [01:16<25:19,  7.95s/it]Dataset: data-is-better-together/10k_prompts_ranked:   5%|▌         | 10/200 [01:27<28:05,  8.87s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 11/200 [01:37<28:34,  9.07s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 12/200 [01:38<21:13,  6.77s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▋         | 13/200 [01:42<17:52,  5.74s/it]Dataset: data-is-better-together/10k_prompts_ranked:   7%|▋         | 14/200 [01:51<21:08,  6.82s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 15/200 [02:03<25:29,  8.27s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 16/200 [02:12<26:06,  8.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 17/200 [02:23<28:09,  9.23s/it]Dataset: data-is-better-together/10k_prompts_ranked:   9%|▉         | 18/200 [02:26<22:20,  7.36s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|▉         | 19/200 [02:33<22:00,  7.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 20/200 [02:34<16:30,  5.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 21/200 [02:39<16:08,  5.41s/it]Dataset: data-is-better-together/10k_prompts_ranked:  11%|█         | 22/200 [02:44<15:08,  5.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 23/200 [02:50<16:12,  5.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 24/200 [03:01<20:55,  7.13s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▎        | 25/200 [03:10<22:19,  7.66s/it]Dataset: data-is-better-together/10k_prompts_ranked:  13%|█▎        | 26/200 [03:21<25:06,  8.66s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▎        | 27/200 [03:25<20:51,  7.23s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 28/200 [03:34<22:43,  7.93s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 29/200 [03:45<25:03,  8.79s/it]Dataset: data-is-better-together/10k_prompts_ranked:  15%|█▌        | 30/200 [03:51<22:09,  7.82s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 31/200 [03:57<21:06,  7.49s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 32/200 [04:08<23:54,  8.54s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▋        | 33/200 [04:19<25:53,  9.30s/it]Dataset: data-is-better-together/10k_prompts_ranked:  17%|█▋        | 34/200 [04:30<27:05,  9.79s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 35/200 [04:38<24:45,  9.00s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 36/200 [04:49<26:13,  9.60s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 37/200 [04:57<24:49,  9.14s/it]Dataset: data-is-better-together/10k_prompts_ranked:  19%|█▉        | 38/200 [05:08<26:16,  9.73s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|█▉        | 39/200 [05:10<20:27,  7.62s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 40/200 [05:17<19:18,  7.24s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 41/200 [05:21<16:36,  6.27s/it]Dataset: data-is-better-together/10k_prompts_ranked:  21%|██        | 42/200 [05:32<20:23,  7.75s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 43/200 [05:43<22:32,  8.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 44/200 [05:54<24:11,  9.30s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▎       | 45/200 [05:58<20:03,  7.76s/it]Dataset: data-is-better-together/10k_prompts_ranked:  23%|██▎       | 46/200 [06:00<15:34,  6.07s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▎       | 47/200 [06:01<11:54,  4.67s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 48/200 [06:03<09:20,  3.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 49/200 [06:12<13:32,  5.38s/it]Dataset: data-is-better-together/10k_prompts_ranked:  25%|██▌       | 50/200 [06:21<16:34,  6.63s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 51/200 [06:31<18:28,  7.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 52/200 [06:40<19:53,  8.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▋       | 53/200 [06:42<15:22,  6.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  27%|██▋       | 54/200 [06:46<13:03,  5.37s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 55/200 [06:46<09:23,  3.88s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 56/200 [06:49<08:57,  3.73s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 57/200 [07:00<14:05,  5.91s/it]Dataset: data-is-better-together/10k_prompts_ranked:  29%|██▉       | 58/200 [07:01<09:53,  4.18s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|██▉       | 59/200 [07:05<10:12,  4.34s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 60/200 [07:16<14:46,  6.33s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 61/200 [07:19<12:09,  5.25s/it]Dataset: data-is-better-together/10k_prompts_ranked:  31%|███       | 62/200 [07:27<14:01,  6.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 63/200 [07:38<17:11,  7.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 64/200 [07:40<13:13,  5.84s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▎      | 65/200 [07:50<15:53,  7.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  33%|███▎      | 66/200 [07:55<14:17,  6.40s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▎      | 67/200 [07:58<12:02,  5.43s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 68/200 [08:03<11:56,  5.43s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 69/200 [08:04<08:50,  4.05s/it]Dataset: data-is-better-together/10k_prompts_ranked:  35%|███▌      | 70/200 [08:05<06:43,  3.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 71/200 [08:12<09:13,  4.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 72/200 [08:17<09:37,  4.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▋      | 73/200 [08:22<09:51,  4.66s/it]Dataset: data-is-better-together/10k_prompts_ranked:  37%|███▋      | 74/200 [08:29<11:09,  5.31s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 75/200 [08:40<14:37,  7.02s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 76/200 [08:51<16:56,  8.20s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 77/200 [08:56<14:53,  7.26s/it]Dataset: data-is-better-together/10k_prompts_ranked:  39%|███▉      | 78/200 [08:57<10:48,  5.32s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|███▉      | 79/200 [09:08<14:10,  7.03s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 80/200 [09:15<13:58,  6.99s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 81/200 [09:26<16:14,  8.19s/it]Dataset: data-is-better-together/10k_prompts_ranked:  41%|████      | 82/200 [09:27<12:15,  6.23s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 83/200 [09:28<08:49,  4.52s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 84/200 [09:34<09:29,  4.91s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▎     | 85/200 [09:40<10:01,  5.23s/it]Dataset: data-is-better-together/10k_prompts_ranked:  43%|████▎     | 86/200 [09:47<11:00,  5.79s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▎     | 87/200 [09:58<13:51,  7.36s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 88/200 [10:06<13:59,  7.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 89/200 [10:16<15:46,  8.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:  45%|████▌     | 90/200 [10:28<17:00,  9.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 91/200 [10:30<13:10,  7.25s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 92/200 [10:34<11:02,  6.13s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▋     | 93/200 [10:37<09:40,  5.42s/it]Dataset: data-is-better-together/10k_prompts_ranked:  47%|████▋     | 94/200 [10:39<07:42,  4.37s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 95/200 [10:41<06:13,  3.56s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 96/200 [10:43<05:23,  3.11s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 97/200 [10:46<05:24,  3.15s/it]Dataset: data-is-better-together/10k_prompts_ranked:  49%|████▉     | 98/200 [10:55<08:20,  4.91s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|████▉     | 99/200 [11:05<10:32,  6.26s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 100/200 [11:05<07:24,  4.45s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 101/200 [11:13<09:24,  5.70s/it]Dataset: data-is-better-together/10k_prompts_ranked:  51%|█████     | 102/200 [11:20<09:33,  5.85s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 103/200 [11:24<08:29,  5.25s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [11:35<11:20,  7.09s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3654 > 2048). Running this sequence through the model will result in indexing errors
Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [11:35<10:41,  6.69s/it]
Average accept_length: 1.41796875
Average accept_length: 1.1953125
Average accept_length: 1.7168141603469849
Average accept_length: 1.1975308656692505
Average accept_length: 1.783333420753479
Average accept_length: 1.18359375
Average accept_length: 1.6080402135849
Average accept_length: 1.1428571939468384
Average accept_length: 2.12890625
Average accept_length: 1.83203125
Average accept_length: 1.3705357313156128
Average accept_length: 0.8333333134651184
Average accept_length: 0.925000011920929
Average accept_length: 1.6834862232208252
Average accept_length: 1.44140625
Average accept_length: 1.23113214969635
Average accept_length: 1.26953125
Average accept_length: 1.3239436149597168
Average accept_length: 1.13178288936615
Average accept_length: 1.193548321723938
Average accept_length: 1.3089430332183838
Average accept_length: 1.2884615659713745
Average accept_length: 1.304635763168335
Average accept_length: 1.02734375
Average accept_length: 1.3157894611358643
Average accept_length: 1.1796875
Average accept_length: 2.11956524848938
Average accept_length: 1.3200000524520874
Average accept_length: 0.99609375
Average accept_length: 1.4318182468414307
Average accept_length: 1.4276728630065918
Average accept_length: 1.34765625
Average accept_length: 1.64453125
Average accept_length: 1.48046875
Average accept_length: 2.0429446697235107
Average accept_length: 1.21484375
Average accept_length: 1.54450261592865
Average accept_length: 1.39453125
Average accept_length: 0.2769230902194977
Average accept_length: 1.6533334255218506
Average accept_length: 1.2631579637527466
Average accept_length: 1.2734375
Average accept_length: 1.1832669973373413
Average accept_length: 1.17578125
Average accept_length: 2.303030252456665
Average accept_length: 2.180000066757202
Average accept_length: 1.151515245437622
Average accept_length: 1.6060606241226196
Average accept_length: 1.2876712083816528
Average accept_length: 1.556053876876831
Average accept_length: 1.2995392084121704
Average accept_length: 2.3499999046325684
Average accept_length: 1.2200000286102295
Average accept_length: 1.454545497894287
Average accept_length: 0.8888888955116272
Average accept_length: 1.8250000476837158
Average accept_length: 1.45703125
Average accept_length: 0.5
Average accept_length: 1.3482143878936768
Average accept_length: 1.0546875
Average accept_length: 1.5230768918991089
Average accept_length: 1.0052083730697632
Average accept_length: 1.17578125
Average accept_length: 1.6666667461395264
Average accept_length: 1.2290748357772827
Average accept_length: 1.204301118850708
Average accept_length: 2.2191779613494873
Average accept_length: 2.1640625
Average accept_length: 1.0526316165924072
Average accept_length: 1.7142857313156128
Average accept_length: 1.567073106765747
Average accept_length: 1.3416666984558105
Average accept_length: 1.4789916276931763
Average accept_length: 1.5493826866149902
Average accept_length: 1.51171875
Average accept_length: 1.8203125
Average accept_length: 1.3916667699813843
Average accept_length: 1.2777777910232544
Average accept_length: 1.25
Average accept_length: 2.044025182723999
Average accept_length: 1.3359375
Average accept_length: 1.0256410837173462
Average accept_length: 1.5
Average accept_length: 1.1594202518463135
Average accept_length: 1.3617020845413208
Average accept_length: 1.8695652484893799
Average accept_length: 1.546875
Average accept_length: 1.9213483333587646
Average accept_length: 1.33984375
Average accept_length: 1.80078125
Average accept_length: 2.2333333492279053
Average accept_length: 1.547619104385376
Average accept_length: 2.0224719047546387
Average accept_length: 1.1111111640930176
Average accept_length: 1.4871795177459717
Average accept_length: 1.1224489212036133
Average accept_length: 1.3246753215789795
Average accept_length: 1.6161137819290161
Average accept_length: 1.0904977321624756
Average accept_length: 1.5
Average accept_length: 1.613861322402954
Average accept_length: 1.451388955116272
Average accept_length: 1.2826087474822998
Average accept_length: 1.90625
Traceback (most recent call last):
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 163, in <module>
    main(args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 147, in main
    metrics = evaluate_dataset(model, args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 106, in evaluate_dataset
    completion = model.generate(prompt, max_steps=getattr(args, "max_new_tokens", 256))
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/sd/HandEval/models/medusa_wrapper.py", line 64, in generate
    return m_generate(
  File "/home/ubuntu/sd/HandEval/models/medusa_wrapper.py", line 22, in m_generate
    for chunk in gen_iter:
  File "/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py", line 307, in medusa_generate
    medusa_logits, logits = initialize_medusa(
  File "/home/ubuntu/sd/Medusa/medusa/model/utils.py", line 146, in initialize_medusa
    medusa_logits, outputs, logits = model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py", line 204, in forward
    outputs = self.base_model.model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 930, in forward
    layer_outputs = decoder_layer(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 625, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 363, in forward
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacity of 79.10 GiB of which 901.75 MiB is free. Including non-PyTorch memory, this process has 78.21 GiB memory in use. Of the allocated memory 76.84 GiB is allocated by PyTorch, and 727.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
