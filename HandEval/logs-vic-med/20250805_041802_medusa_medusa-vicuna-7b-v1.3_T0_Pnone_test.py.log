LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.09s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-7b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
HumanEval Tasks:   0%|          | 0/164 [00:00<?, ?it/s]HumanEval Tasks:   1%|          | 1/164 [00:00<02:34,  1.06it/s]HumanEval Tasks:   1%|          | 2/164 [00:01<01:51,  1.45it/s]HumanEval Tasks:   2%|â–         | 3/164 [00:01<01:26,  1.86it/s]HumanEval Tasks:   2%|â–         | 4/164 [00:02<01:20,  1.99it/s]HumanEval Tasks:   3%|â–Ž         | 5/164 [00:03<02:00,  1.32it/s]HumanEval Tasks:   4%|â–Ž         | 6/164 [00:04<02:20,  1.13it/s]HumanEval Tasks:   4%|â–         | 7/164 [00:05<02:28,  1.05it/s]HumanEval Tasks:   5%|â–         | 8/164 [00:06<02:32,  1.03it/s]HumanEval Tasks:   5%|â–Œ         | 9/164 [00:07<02:18,  1.12it/s]HumanEval Tasks:   6%|â–Œ         | 10/164 [00:08<02:08,  1.20it/s]HumanEval Tasks:   7%|â–‹         | 11/164 [00:09<02:47,  1.10s/it]HumanEval Tasks:   7%|â–‹         | 12/164 [00:10<02:27,  1.03it/s]HumanEval Tasks:   8%|â–Š         | 13/164 [00:10<02:04,  1.21it/s]HumanEval Tasks:   9%|â–Š         | 14/164 [00:11<02:04,  1.21it/s]HumanEval Tasks:   9%|â–‰         | 15/164 [00:12<01:44,  1.43it/s]HumanEval Tasks:  10%|â–‰         | 16/164 [00:12<01:23,  1.78it/s]HumanEval Tasks:  10%|â–ˆ         | 17/164 [00:13<01:25,  1.72it/s]HumanEval Tasks:  11%|â–ˆ         | 18/164 [00:13<01:30,  1.62it/s]HumanEval Tasks:  12%|â–ˆâ–        | 19/164 [00:14<01:28,  1.64it/s]HumanEval Tasks:  12%|â–ˆâ–        | 20/164 [00:14<01:14,  1.94it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 21/164 [00:15<01:24,  1.69it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 22/164 [00:16<01:55,  1.23it/s]HumanEval Tasks:  14%|â–ˆâ–        | 23/164 [00:17<01:35,  1.48it/s]HumanEval Tasks:  15%|â–ˆâ–        | 24/164 [00:17<01:13,  1.91it/s]HumanEval Tasks:  15%|â–ˆâ–Œ        | 25/164 [00:17<01:11,  1.94it/s]HumanEval Tasks:  16%|â–ˆâ–Œ        | 26/164 [00:18<01:06,  2.06it/s]HumanEval Tasks:  16%|â–ˆâ–‹        | 27/164 [00:18<01:02,  2.20it/s]HumanEval Tasks:  17%|â–ˆâ–‹        | 28/164 [00:18<00:52,  2.57it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 29/164 [00:19<00:44,  3.05it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 30/164 [00:19<01:05,  2.05it/s]HumanEval Tasks:  19%|â–ˆâ–‰        | 31/164 [00:20<00:55,  2.39it/s]HumanEval Tasks:  20%|â–ˆâ–‰        | 32/164 [00:21<01:20,  1.64it/s]HumanEval Tasks:  20%|â–ˆâ–ˆ        | 33/164 [00:23<02:24,  1.10s/it]HumanEval Tasks:  21%|â–ˆâ–ˆ        | 34/164 [00:24<02:03,  1.05it/s]HumanEval Tasks:  21%|â–ˆâ–ˆâ–       | 35/164 [00:24<01:33,  1.38it/s]HumanEval Tasks:  22%|â–ˆâ–ˆâ–       | 36/164 [00:24<01:29,  1.42it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 37/164 [00:25<01:37,  1.30it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 38/164 [00:26<01:17,  1.63it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 39/164 [00:26<01:21,  1.53it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 40/164 [00:28<02:07,  1.03s/it]HumanEval Tasks:  25%|â–ˆâ–ˆâ–Œ       | 41/164 [00:29<01:50,  1.11it/s]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 42/164 [00:30<01:48,  1.13it/s]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 43/164 [00:30<01:23,  1.45it/s]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 44/164 [00:30<01:16,  1.57it/s]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 45/164 [00:31<01:03,  1.87it/s]HumanEval Tasks:  28%|â–ˆâ–ˆâ–Š       | 46/164 [00:31<00:51,  2.30it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–Š       | 47/164 [00:31<00:52,  2.22it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–‰       | 48/164 [00:32<00:59,  1.96it/s]HumanEval Tasks:  30%|â–ˆâ–ˆâ–‰       | 49/164 [00:33<01:07,  1.70it/s]HumanEval Tasks:  30%|â–ˆâ–ˆâ–ˆ       | 50/164 [00:34<01:20,  1.42it/s]HumanEval Tasks:  31%|â–ˆâ–ˆâ–ˆ       | 51/164 [00:34<01:12,  1.57it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/164 [00:35<01:03,  1.76it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 53/164 [00:35<00:53,  2.08it/s]HumanEval Tasks:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/164 [00:35<00:41,  2.62it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 55/164 [00:36<00:41,  2.63it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–      | 56/164 [00:36<00:37,  2.86it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–      | 57/164 [00:36<00:39,  2.73it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/164 [00:37<01:00,  1.76it/s]HumanEval Tasks:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 59/164 [00:37<00:49,  2.12it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/164 [00:38<00:48,  2.16it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 61/164 [00:38<00:41,  2.48it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/164 [00:39<00:42,  2.38it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 63/164 [00:39<00:41,  2.44it/s]HumanEval Tasks:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 64/164 [00:39<00:40,  2.44it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 65/164 [00:40<00:40,  2.45it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/164 [00:40<00:35,  2.74it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 67/164 [00:40<00:33,  2.86it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/164 [00:41<00:33,  2.89it/s]HumanEval Tasks:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 69/164 [00:41<00:34,  2.77it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/164 [00:42<00:43,  2.14it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 71/164 [00:42<00:35,  2.61it/s]HumanEval Tasks:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/164 [00:43<00:37,  2.46it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/164 [00:43<00:39,  2.30it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/164 [00:44<00:47,  1.90it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 75/164 [00:44<00:41,  2.15it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/164 [00:45<00:40,  2.16it/s]HumanEval Tasks:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 77/164 [00:45<00:35,  2.45it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/164 [00:45<00:38,  2.21it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 79/164 [00:48<01:40,  1.18s/it]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/164 [00:49<01:21,  1.03it/s]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 81/164 [00:50<01:31,  1.11s/it]HumanEval Tasks:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/164 [00:52<01:36,  1.18s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/164 [00:53<01:38,  1.22s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 84/164 [00:53<01:18,  1.01it/s]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/164 [00:54<01:02,  1.27it/s]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 86/164 [00:54<00:52,  1.49it/s]HumanEval Tasks:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/164 [00:54<00:47,  1.63it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 88/164 [00:55<00:40,  1.90it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/164 [00:55<00:32,  2.34it/s]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/164 [00:55<00:29,  2.53it/s]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/164 [00:56<00:25,  2.91it/s]HumanEval Tasks:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 92/164 [00:56<00:26,  2.67it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/164 [00:56<00:27,  2.57it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 94/164 [00:57<00:39,  1.75it/s]HumanEval Tasks:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/164 [00:58<00:42,  1.63it/s]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 96/164 [00:59<00:37,  1.81it/s]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/164 [00:59<00:38,  1.74it/s]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 98/164 [00:59<00:30,  2.13it/s]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/164 [01:00<00:31,  2.05it/s]HumanEval Tasks:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 100/164 [01:00<00:27,  2.32it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/164 [01:01<00:29,  2.17it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 102/164 [01:01<00:23,  2.65it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/164 [01:01<00:20,  2.98it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 104/164 [01:02<00:24,  2.43it/s]HumanEval Tasks:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/164 [01:02<00:23,  2.47it/s]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/164 [01:02<00:20,  2.89it/s]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/164 [01:03<00:21,  2.67it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 108/164 [01:03<00:23,  2.39it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/164 [01:04<00:21,  2.61it/s]HumanEval Tasks:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 110/164 [01:04<00:20,  2.59it/s]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/164 [01:05<00:23,  2.29it/s]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 112/164 [01:05<00:21,  2.47it/s]HumanEval Tasks:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/164 [01:05<00:23,  2.20it/s]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 114/164 [01:06<00:24,  2.06it/s]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/164 [01:06<00:22,  2.17it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 116/164 [01:07<00:23,  2.07it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/164 [01:07<00:20,  2.28it/s]HumanEval Tasks:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 118/164 [01:08<00:20,  2.20it/s]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/164 [01:10<00:40,  1.11it/s]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 120/164 [01:10<00:36,  1.20it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/164 [01:11<00:27,  1.54it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/164 [01:11<00:25,  1.62it/s]HumanEval Tasks:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/164 [01:12<00:23,  1.77it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 124/164 [01:12<00:20,  1.91it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 125/164 [01:13<00:24,  1.60it/s]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 126/164 [01:13<00:20,  1.89it/s]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 127/164 [01:13<00:16,  2.26it/s]HumanEval Tasks:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 128/164 [01:14<00:22,  1.62it/s]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 129/164 [01:15<00:19,  1.83it/s]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 130/164 [01:16<00:29,  1.14it/s]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 131/164 [01:18<00:33,  1.01s/it]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 132/164 [01:18<00:27,  1.18it/s]HumanEval Tasks:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 133/164 [01:19<00:26,  1.15it/s]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 134/164 [01:19<00:20,  1.48it/s]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 135/164 [01:20<00:17,  1.68it/s]HumanEval Tasks:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 136/164 [01:20<00:15,  1.76it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 137/164 [01:21<00:14,  1.88it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 138/164 [01:23<00:24,  1.05it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139/164 [01:23<00:20,  1.24it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 140/164 [01:23<00:15,  1.54it/s]HumanEval Tasks:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 141/164 [01:24<00:12,  1.85it/s]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 142/164 [01:25<00:13,  1.60it/s]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 143/164 [01:25<00:12,  1.74it/s]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 144/164 [01:26<00:12,  1.55it/s]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 145/164 [01:26<00:11,  1.69it/s]HumanEval Tasks:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/164 [01:27<00:09,  1.97it/s]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 147/164 [01:27<00:09,  1.86it/s]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 148/164 [01:28<00:09,  1.62it/s]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 149/164 [01:29<00:11,  1.27it/s]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 150/164 [01:30<00:09,  1.53it/s]HumanEval Tasks:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 151/164 [01:30<00:08,  1.62it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 152/164 [01:30<00:06,  1.92it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 153/164 [01:31<00:05,  1.89it/s]HumanEval Tasks:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 154/164 [01:31<00:04,  2.03it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 155/164 [01:32<00:04,  1.81it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 156/164 [01:32<00:04,  1.99it/s]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 157/164 [01:34<00:05,  1.26it/s]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 158/164 [01:34<00:04,  1.46it/s]HumanEval Tasks:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 159/164 [01:35<00:03,  1.63it/s]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 160/164 [01:35<00:02,  1.60it/s]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 161/164 [01:40<00:05,  1.88s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 162/164 [01:41<00:03,  1.53s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 163/164 [01:41<00:01,  1.17s/it]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [01:42<00:00,  1.10it/s]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [01:42<00:00,  1.61it/s]
Average accept_length: 1.4230769872665405
Average accept_length: 0.7307692766189575
Average accept_length: 1.7222222089767456
Average accept_length: 1.7826087474822998
Average accept_length: 1.4920635223388672
Average accept_length: 1.8166667222976685
Average accept_length: 1.4736841917037964
Average accept_length: 1.6363636255264282
Average accept_length: 1.894736886024475
Average accept_length: 1.388888955116272
Average accept_length: 1.5222222805023193
Average accept_length: 2.200000047683716
Average accept_length: 1.461538553237915
Average accept_length: 1.3181818723678589
Average accept_length: 1.7619048357009888
Average accept_length: 2.0
Average accept_length: 1.9090909957885742
Average accept_length: 1.8648649454116821
Average accept_length: 1.6774193048477173
Average accept_length: 1.7333334684371948
Average accept_length: 2.02439022064209
Average accept_length: 1.6619718074798584
Average accept_length: 1.7777777910232544
Average accept_length: 2.0
Average accept_length: 1.846153974533081
Average accept_length: 2.0
Average accept_length: 1.6315789222717285
Average accept_length: 1.5833333730697632
Average accept_length: 2.1111111640930176
Average accept_length: 1.6521739959716797
Average accept_length: 1.6153846979141235
Average accept_length: 2.0701754093170166
Average accept_length: 1.4833334684371948
Average accept_length: 1.6875
Average accept_length: 1.2000000476837158
Average accept_length: 1.9428571462631226
Average accept_length: 1.9387754201889038
Average accept_length: 1.692307710647583
Average accept_length: 1.1750000715255737
Average accept_length: 1.843137264251709
Average accept_length: 1.8064515590667725
Average accept_length: 1.6739131212234497
Average accept_length: 1.6666667461395264
Average accept_length: 2.1481480598449707
Average accept_length: 1.6000001430511475
Average accept_length: 1.600000023841858
Average accept_length: 1.8399999141693115
Average accept_length: 1.9411765336990356
Average accept_length: 1.600000023841858
Average accept_length: 1.5882353782653809
Average accept_length: 1.0799999237060547
Average accept_length: 1.3333333730697632
Average accept_length: 1.571428656578064
Average accept_length: 2.2857143878936768
Average accept_length: 1.399999976158142
Average accept_length: 2.2142858505249023
Average accept_length: 1.571428656578064
Average accept_length: 1.545454502105713
Average accept_length: 1.9166667461395264
Average accept_length: 2.04347825050354
Average accept_length: 2.461538553237915
Average accept_length: 1.4166667461395264
Average accept_length: 0.800000011920929
Average accept_length: 1.8571429252624512
Average accept_length: 1.7619048357009888
Average accept_length: 1.2307692766189575
Average accept_length: 1.8125
Average accept_length: 1.4117647409439087
Average accept_length: 1.5500000715255737
Average accept_length: 1.605263113975525
Average accept_length: 1.888888955116272
Average accept_length: 1.7083333730697632
Average accept_length: 1.884615421295166
Average accept_length: 2.1794872283935547
Average accept_length: 1.8125
Average accept_length: 1.7916667461395264
Average accept_length: 2.642857313156128
Average accept_length: 2.275861978530884
Average accept_length: 1.605263113975525
Average accept_length: 1.461538553237915
Average accept_length: 1.6800000667572021
Average accept_length: 1.6527777910232544
Average accept_length: 1.7391304969787598
Average accept_length: 1.8695652484893799
Average accept_length: 1.7058823108673096
Average accept_length: 1.7000000476837158
Average accept_length: 2.0399999618530273
Average accept_length: 1.625
Average accept_length: 1.6666666269302368
Average accept_length: 1.5
Average accept_length: 2.0
Average accept_length: 1.43478262424469
Average accept_length: 1.454545497894287
Average accept_length: 1.3396226167678833
Average accept_length: 1.7894736528396606
Average accept_length: 1.8095238208770752
Average accept_length: 2.060606002807617
Average accept_length: 1.454545497894287
Average accept_length: 1.7857143878936768
Average accept_length: 1.8666667938232422
Average accept_length: 1.8928571939468384
Average accept_length: 1.7777777910232544
Average accept_length: 1.5833333730697632
Average accept_length: 1.225806474685669
Average accept_length: 1.7000000476837158
Average accept_length: 2.0
Average accept_length: 2.13043475151062
Average accept_length: 2.346153974533081
Average accept_length: 2.200000047683716
Average accept_length: 2.25
Average accept_length: 1.7586207389831543
Average accept_length: 2.3529412746429443
Average accept_length: 1.2666667699813843
Average accept_length: 2.2068965435028076
Average accept_length: 1.9047619104385376
Average accept_length: 2.222222328186035
Average accept_length: 1.5882352590560913
Average accept_length: 1.6399999856948853
Average accept_length: 1.4380953311920166
Average accept_length: 1.399999976158142
Average accept_length: 1.7272727489471436
Average accept_length: 1.25
Average accept_length: 1.95652174949646
Average accept_length: 1.954545497894287
Average accept_length: 1.5777778625488281
Average accept_length: 1.8666667938232422
Average accept_length: 1.5833333730697632
Average accept_length: 1.814814805984497
Average accept_length: 1.600000023841858
Average accept_length: 1.7701148986816406
Average accept_length: 1.7000000476837158
Average accept_length: 1.9166667461395264
Average accept_length: 1.4897959232330322
Average accept_length: 1.454545497894287
Average accept_length: 1.0952380895614624
Average accept_length: 1.692307710647583
Average accept_length: 1.56521737575531
Average accept_length: 1.3689320087432861
Average accept_length: 2.2083334922790527
Average accept_length: 2.0714287757873535
Average accept_length: 1.6000001430511475
Average accept_length: 2.0
Average accept_length: 2.2916667461395264
Average accept_length: 2.0
Average accept_length: 1.875
Average accept_length: 1.625
Average accept_length: 1.71875
Average accept_length: 1.8139535188674927
Average accept_length: 1.2698413133621216
Average accept_length: 2.2941176891326904
Average accept_length: 2.1785714626312256
Average accept_length: 1.8000000715255737
Average accept_length: 1.8928571939468384
Average accept_length: 1.649999976158142
Average accept_length: 1.6944444179534912
Average accept_length: 1.100000023841858
Average accept_length: 1.4230769872665405
Average accept_length: 1.5909091234207153
Average accept_length: 1.43478262424469
Average accept_length: 1.058823585510254
Average accept_length: 1.44140625
Average accept_length: 1.6315789222717285
Average accept_length: 1.9375
Average accept_length: 1.6666667461395264
ðŸ”„ Completions saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-7b-v1.3/humaneval_samples.jsonl

âœ…  Results saved to /home/ubuntu/sd/HandEval/outputs/medusa-vicuna-7b-v1.3/humaneval_metrics.json
{
  "num_samples": 164,
  "samples_file": "outputs/medusa-vicuna-7b-v1.3/humaneval_samples.jsonl"
}
