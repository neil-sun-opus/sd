LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.53s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.35s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-13b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
HumanEval Tasks:   0%|          | 0/164 [00:00<?, ?it/s]HumanEval Tasks:   1%|          | 1/164 [00:01<05:16,  1.94s/it]HumanEval Tasks:   1%|          | 2/164 [00:03<04:18,  1.59s/it]HumanEval Tasks:   2%|â–         | 3/164 [00:03<02:49,  1.05s/it]HumanEval Tasks:   2%|â–         | 4/164 [00:04<02:11,  1.21it/s]HumanEval Tasks:   3%|â–Ž         | 5/164 [00:05<03:06,  1.17s/it]HumanEval Tasks:   4%|â–Ž         | 6/164 [00:06<02:50,  1.08s/it]HumanEval Tasks:   4%|â–         | 7/164 [00:08<03:13,  1.23s/it]HumanEval Tasks:   5%|â–         | 8/164 [00:09<03:15,  1.26s/it]HumanEval Tasks:   5%|â–Œ         | 9/164 [00:10<02:47,  1.08s/it]HumanEval Tasks:   6%|â–Œ         | 10/164 [00:11<02:35,  1.01s/it]HumanEval Tasks:   7%|â–‹         | 11/164 [00:13<03:29,  1.37s/it]HumanEval Tasks:   7%|â–‹         | 12/164 [00:14<03:06,  1.22s/it]HumanEval Tasks:   8%|â–Š         | 13/164 [00:15<02:57,  1.17s/it]HumanEval Tasks:   9%|â–Š         | 14/164 [00:16<03:00,  1.20s/it]HumanEval Tasks:   9%|â–‰         | 15/164 [00:17<02:34,  1.04s/it]HumanEval Tasks:  10%|â–‰         | 16/164 [00:17<02:03,  1.20it/s]HumanEval Tasks:  10%|â–ˆ         | 17/164 [00:18<01:40,  1.47it/s]HumanEval Tasks:  11%|â–ˆ         | 18/164 [00:19<01:57,  1.25it/s]HumanEval Tasks:  12%|â–ˆâ–        | 19/164 [00:19<01:47,  1.34it/s]HumanEval Tasks:  12%|â–ˆâ–        | 20/164 [00:20<01:42,  1.40it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 21/164 [00:21<02:07,  1.12it/s]HumanEval Tasks:  13%|â–ˆâ–Ž        | 22/164 [00:23<02:40,  1.13s/it]HumanEval Tasks:  14%|â–ˆâ–        | 23/164 [00:24<02:26,  1.04s/it]HumanEval Tasks:  15%|â–ˆâ–        | 24/164 [00:24<01:50,  1.27it/s]HumanEval Tasks:  15%|â–ˆâ–Œ        | 25/164 [00:24<01:37,  1.42it/s]HumanEval Tasks:  16%|â–ˆâ–Œ        | 26/164 [00:26<01:58,  1.17it/s]HumanEval Tasks:  16%|â–ˆâ–‹        | 27/164 [00:26<01:52,  1.22it/s]HumanEval Tasks:  17%|â–ˆâ–‹        | 28/164 [00:27<01:40,  1.35it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 29/164 [00:27<01:20,  1.68it/s]HumanEval Tasks:  18%|â–ˆâ–Š        | 30/164 [00:28<01:26,  1.55it/s]HumanEval Tasks:  19%|â–ˆâ–‰        | 31/164 [00:28<01:11,  1.86it/s]HumanEval Tasks:  20%|â–ˆâ–‰        | 32/164 [00:30<01:45,  1.26it/s]HumanEval Tasks:  20%|â–ˆâ–ˆ        | 33/164 [00:32<03:06,  1.42s/it]HumanEval Tasks:  21%|â–ˆâ–ˆ        | 34/164 [00:33<02:36,  1.20s/it]HumanEval Tasks:  21%|â–ˆâ–ˆâ–       | 35/164 [00:34<02:06,  1.02it/s]HumanEval Tasks:  22%|â–ˆâ–ˆâ–       | 36/164 [00:34<01:59,  1.07it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 37/164 [00:35<01:54,  1.11it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 38/164 [00:36<01:38,  1.27it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 39/164 [00:37<02:02,  1.02it/s]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 40/164 [00:43<05:15,  2.54s/it]HumanEval Tasks:  25%|â–ˆâ–ˆâ–Œ       | 41/164 [00:44<04:11,  2.05s/it]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 42/164 [00:46<04:02,  1.99s/it]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 43/164 [00:46<03:01,  1.50s/it]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 44/164 [00:47<02:39,  1.33s/it]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 45/164 [00:48<02:00,  1.01s/it]HumanEval Tasks:  28%|â–ˆâ–ˆâ–Š       | 46/164 [00:48<01:44,  1.13it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–Š       | 47/164 [00:50<02:02,  1.05s/it]HumanEval Tasks:  29%|â–ˆâ–ˆâ–‰       | 48/164 [00:51<02:00,  1.03s/it]HumanEval Tasks:  30%|â–ˆâ–ˆâ–‰       | 49/164 [00:51<01:32,  1.25it/s]HumanEval Tasks:  30%|â–ˆâ–ˆâ–ˆ       | 50/164 [00:52<01:46,  1.07it/s]HumanEval Tasks:  31%|â–ˆâ–ˆâ–ˆ       | 51/164 [00:53<01:38,  1.15it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/164 [00:53<01:25,  1.31it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 53/164 [00:55<01:43,  1.07it/s]HumanEval Tasks:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/164 [00:55<01:18,  1.41it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 55/164 [00:56<01:13,  1.49it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–      | 56/164 [00:56<01:02,  1.74it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–      | 57/164 [00:57<01:07,  1.58it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/164 [00:57<01:03,  1.67it/s]HumanEval Tasks:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 59/164 [00:59<01:37,  1.08it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/164 [01:00<01:31,  1.14it/s]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 61/164 [01:00<01:12,  1.42it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/164 [01:02<01:38,  1.03it/s]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 63/164 [01:03<01:44,  1.03s/it]HumanEval Tasks:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 64/164 [01:03<01:29,  1.12it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 65/164 [01:04<01:17,  1.28it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/164 [01:05<01:31,  1.07it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 67/164 [01:05<01:12,  1.34it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/164 [01:06<01:12,  1.33it/s]HumanEval Tasks:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 69/164 [01:07<01:20,  1.18it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/164 [01:08<01:20,  1.17it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 71/164 [01:09<01:09,  1.35it/s]HumanEval Tasks:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/164 [01:09<01:11,  1.29it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/164 [01:10<01:12,  1.26it/s]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/164 [01:12<01:30,  1.00s/it]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 75/164 [01:12<01:17,  1.15it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/164 [01:13<01:17,  1.14it/s]HumanEval Tasks:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 77/164 [01:13<00:59,  1.46it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/164 [01:14<00:52,  1.63it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 79/164 [01:14<00:50,  1.69it/s]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/164 [01:15<00:52,  1.61it/s]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 81/164 [01:16<00:56,  1.46it/s]HumanEval Tasks:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/164 [01:18<01:26,  1.05s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/164 [01:19<01:34,  1.17s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 84/164 [01:20<01:28,  1.10s/it]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/164 [01:22<01:45,  1.34s/it]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 86/164 [01:23<01:21,  1.05s/it]HumanEval Tasks:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/164 [01:23<01:04,  1.19it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 88/164 [01:25<01:30,  1.19s/it]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/164 [01:25<01:15,  1.00s/it]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/164 [01:26<01:05,  1.12it/s]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/164 [01:26<00:50,  1.44it/s]HumanEval Tasks:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 92/164 [01:27<00:55,  1.31it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/164 [01:28<00:47,  1.49it/s]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 94/164 [01:29<01:03,  1.11it/s]HumanEval Tasks:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/164 [01:33<02:05,  1.82s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 96/164 [01:36<02:24,  2.12s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/164 [01:37<01:54,  1.71s/it]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 98/164 [01:37<01:23,  1.26s/it]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/164 [01:37<01:07,  1.04s/it]HumanEval Tasks:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 100/164 [01:38<00:53,  1.19it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/164 [01:38<00:46,  1.35it/s]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 102/164 [01:39<00:37,  1.66it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/164 [01:39<00:32,  1.89it/s]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 104/164 [01:40<00:35,  1.69it/s]HumanEval Tasks:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/164 [01:41<00:50,  1.17it/s]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/164 [01:43<00:58,  1.01s/it]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/164 [01:43<00:48,  1.17it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 108/164 [01:44<00:43,  1.30it/s]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/164 [01:44<00:43,  1.26it/s]HumanEval Tasks:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 110/164 [01:48<01:34,  1.75s/it]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/164 [01:49<01:07,  1.28s/it]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 112/164 [01:49<00:54,  1.05s/it]HumanEval Tasks:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/164 [01:50<00:44,  1.14it/s]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 114/164 [01:51<00:55,  1.11s/it]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/164 [01:52<00:47,  1.04it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 116/164 [01:53<00:44,  1.07it/s]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/164 [01:54<00:45,  1.04it/s]HumanEval Tasks:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 118/164 [01:54<00:38,  1.21it/s]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/164 [01:56<00:51,  1.14s/it]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 120/164 [01:57<00:43,  1.00it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/164 [01:58<00:41,  1.03it/s]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/164 [01:58<00:33,  1.26it/s]HumanEval Tasks:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/164 [01:59<00:38,  1.08it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 124/164 [02:02<01:00,  1.52s/it]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 125/164 [02:05<01:10,  1.82s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 126/164 [02:06<00:59,  1.57s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 127/164 [02:06<00:45,  1.22s/it]HumanEval Tasks:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 128/164 [02:08<00:45,  1.26s/it]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 129/164 [02:08<00:36,  1.05s/it]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 130/164 [02:12<01:02,  1.83s/it]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 131/164 [02:13<00:55,  1.69s/it]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 132/164 [02:14<00:43,  1.37s/it]HumanEval Tasks:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 133/164 [02:15<00:38,  1.25s/it]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 134/164 [02:15<00:28,  1.04it/s]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 135/164 [02:16<00:24,  1.16it/s]HumanEval Tasks:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 136/164 [02:16<00:21,  1.32it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 137/164 [02:17<00:17,  1.50it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 138/164 [02:18<00:21,  1.23it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139/164 [02:18<00:19,  1.26it/s]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 140/164 [02:20<00:22,  1.07it/s]HumanEval Tasks:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 141/164 [02:21<00:26,  1.16s/it]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 142/164 [02:23<00:31,  1.42s/it]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 143/164 [02:24<00:24,  1.17s/it]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 144/164 [02:25<00:19,  1.00it/s]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 145/164 [02:27<00:26,  1.41s/it]Average accept_length after 59 steps : 2.0847458839416504
Average accept_length after 56 steps : 1.5178571939468384
Average accept_length after 16 steps : 1.75
Average accept_length after 19 steps : 2.21052622795105
Average accept_length after 74 steps : 1.648648738861084
Average accept_length after 37 steps : 2.1891891956329346
Average accept_length after 64 steps : 1.390625
Average accept_length after 54 steps : 1.5
Average accept_length after 29 steps : 2.2068965435028076
Average accept_length after 35 steps : 2.200000047683716
Average accept_length after 90 steps : 1.7222222089767456
Average accept_length after 37 steps : 2.054054021835327
Average accept_length after 44 steps : 1.6590909957885742
Average accept_length after 52 steps : 1.711538553237915
Average accept_length after 27 steps : 1.888888955116272
Average accept_length after 14 steps : 1.7142858505249023
Average accept_length after 13 steps : 1.769230842590332
Average accept_length after 45 steps : 2.1777777671813965
Average accept_length after 25 steps : 2.0
Average accept_length after 26 steps : 1.884615421295166
Average accept_length after 54 steps : 1.7222222089767456
Average accept_length after 70 steps : 1.9142857789993286
Average accept_length after 35 steps : 2.028571367263794
Average accept_length after 7 steps : 2.4285714626312256
Average accept_length after 21 steps : 1.523809552192688
Average accept_length after 50 steps : 1.5199999809265137
Average accept_length after 30 steps : 2.433333396911621
Average accept_length after 22 steps : 1.6363637447357178
Average accept_length after 10 steps : 2.4000000953674316
Average accept_length after 31 steps : 1.9354838132858276
Average accept_length after 11 steps : 2.090909242630005
Average accept_length after 58 steps : 2.0172414779663086
Average accept_length after 119 steps : 1.403361439704895
Average accept_length after 28 steps : 2.0
Average accept_length after 18 steps : 2.444444417953491
Average accept_length after 34 steps : 1.9411765336990356
Average accept_length after 34 steps : 2.2647058963775635
Average accept_length after 21 steps : 1.9047619104385376
Average accept_length after 59 steps : 1.4745762348175049
Average accept_length after 256 steps : 1.6953125
Average accept_length after 36 steps : 2.1388888359069824
Average accept_length after 77 steps : 1.454545497894287
Average accept_length after 14 steps : 1.5000001192092896
Average accept_length after 38 steps : 2.0526316165924072
Average accept_length after 11 steps : 1.6363637447357178
Average accept_length after 24 steps : 1.625
Average accept_length after 59 steps : 1.881355881690979
Average accept_length after 42 steps : 1.7142857313156128
Average accept_length after 10 steps : 1.8000000715255737
Average accept_length after 52 steps : 1.5384615659713745
Average accept_length after 29 steps : 1.3448275327682495
Average accept_length after 20 steps : 1.7000000476837158
Average accept_length after 56 steps : 1.8392858505249023
Average accept_length after 7 steps : 2.2857143878936768
Average accept_length after 23 steps : 1.39130437374115
Average accept_length after 14 steps : 2.4285714626312256
Average accept_length after 31 steps : 1.9032257795333862
Average accept_length after 21 steps : 1.9047619104385376
Average accept_length after 71 steps : 1.8873238563537598
Average accept_length after 31 steps : 1.7096773386001587
Average accept_length after 11 steps : 2.0
Average accept_length after 66 steps : 1.621212124824524
Average accept_length after 49 steps : 1.5102040767669678
Average accept_length after 23 steps : 1.56521737575531
Average accept_length after 21 steps : 1.9047619104385376
Average accept_length after 54 steps : 1.4629629850387573
Average accept_length after 12 steps : 1.5833333730697632
Average accept_length after 31 steps : 1.612903118133545
Average accept_length after 43 steps : 1.6511627435684204
Average accept_length after 36 steps : 1.8611111640930176
Average accept_length after 19 steps : 2.21052622795105
Average accept_length after 35 steps : 1.7999999523162842
Average accept_length after 34 steps : 1.8235293626785278
Average accept_length after 61 steps : 1.9672130346298218
Average accept_length after 22 steps : 1.9090909957885742
Average accept_length after 37 steps : 2.2432432174682617
Average accept_length after 9 steps : 2.1111111640930176
Average accept_length after 18 steps : 2.1111111640930176
Average accept_length after 21 steps : 2.0
Average accept_length after 28 steps : 1.6071429252624512
Average accept_length after 34 steps : 2.117647171020508
Average accept_length after 77 steps : 1.9090908765792847
Average accept_length after 60 steps : 1.5166667699813843
Average accept_length after 39 steps : 1.6666667461395264
Average accept_length after 79 steps : 1.8607594966888428
Average accept_length after 15 steps : 2.4666666984558105
Average accept_length after 14 steps : 1.5000001192092896
Average accept_length after 82 steps : 1.7804877758026123
Average accept_length after 23 steps : 2.3913044929504395
Average accept_length after 25 steps : 1.4800000190734863
Average accept_length after 9 steps : 2.444444417953491
Average accept_length after 38 steps : 1.394736886024475
Average accept_length after 18 steps : 2.055555582046509
Average accept_length after 60 steps : 1.3666667938232422
Average accept_length after 163 steps : 1.852760672569275
Average accept_length after 117 steps : 1.3076924085617065
Average accept_length after 31 steps : 2.161290168762207
Average accept_length after 8 steps : 1.375
Average accept_length after 21 steps : 2.142857074737549
Average accept_length after 15 steps : 1.8000000715255737
Average accept_length after 20 steps : 1.9500000476837158
Average accept_length after 11 steps : 1.8181818723678589
Average accept_length after 14 steps : 2.4285714626312256
Average accept_length after 30 steps : 1.3000000715255737
Average accept_length after 61 steps : 1.9836064577102661
Average accept_length after 55 steps : 2.0545454025268555
Average accept_length after 20 steps : 2.299999952316284
Average accept_length after 23 steps : 2.3478262424468994
Average accept_length after 35 steps : 1.9428571462631226
Average accept_length after 163 steps : 1.6871165037155151
Average accept_length after 7 steps : 2.0
Average accept_length after 20 steps : 2.0
Average accept_length after 19 steps : 1.894736886024475
Average accept_length after 68 steps : 1.2647058963775635
Average accept_length after 25 steps : 2.1599998474121094
Average accept_length after 34 steps : 2.058823585510254
Average accept_length after 43 steps : 1.9069766998291016
Average accept_length after 20 steps : 1.9500000476837158
Average accept_length after 78 steps : 1.5641025304794312
Average accept_length after 26 steps : 1.5
Average accept_length after 37 steps : 1.8378379344940186
Average accept_length after 15 steps : 2.6000001430511475
Average accept_length after 51 steps : 1.9607844352722168
Average accept_length after 120 steps : 1.5916666984558105
Average accept_length after 104 steps : 1.9230769872665405
Average accept_length after 41 steps : 1.5609755516052246
Average accept_length after 15 steps : 2.200000047683716
Average accept_length after 55 steps : 1.545454502105713
Average accept_length after 23 steps : 2.2608695030212402
Average accept_length after 149 steps : 1.644295334815979
Average accept_length after 55 steps : 1.8545453548431396
Average accept_length after 26 steps : 2.076923131942749
Average accept_length after 40 steps : 1.75
Average accept_length after 10 steps : 1.7000000476837158
Average accept_length after 25 steps : 1.7599999904632568
Average accept_length after 20 steps : 2.299999952316284
Average accept_length after 18 steps : 2.0
Average accept_length after 48 steps : 1.7291667461395264
Average accept_length after 30 steps : 1.933333396911621
Average accept_length after 52 steps : 1.7307692766189575
Average accept_length after 70 steps : 1.3285714387893677
Average accept_length after 83 steps : 1.7590360641479492
Average accept_length after 23 steps : 2.3478262424468994
Average accept_length after 24 steps : 1.9583333730697632
Average accept_length after 98 steps : 1.5204081535339355
HumanEval Tasks:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/164 [02:27<00:20,  1.11s/it]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 147/164 [02:28<00:16,  1.05it/s]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 148/164 [02:29<00:14,  1.08it/s]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 149/164 [02:30<00:15,  1.06s/it]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 150/164 [02:30<00:11,  1.22it/s]HumanEval Tasks:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 151/164 [02:31<00:10,  1.24it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 152/164 [02:32<00:08,  1.47it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 153/164 [02:32<00:07,  1.51it/s]HumanEval Tasks:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 154/164 [02:36<00:16,  1.68s/it]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 155/164 [02:38<00:15,  1.77s/it]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 156/164 [02:39<00:12,  1.53s/it]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 157/164 [02:41<00:11,  1.58s/it]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 158/164 [02:43<00:09,  1.57s/it]HumanEval Tasks:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 159/164 [02:44<00:07,  1.48s/it]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 160/164 [02:44<00:04,  1.19s/it]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 161/164 [02:48<00:05,  1.92s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 162/164 [02:49<00:03,  1.71s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 163/164 [02:50<00:01,  1.56s/it]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [02:51<00:00,  1.31s/it]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [02:51<00:00,  1.05s/it]
Average accept_length after 17 steps : 1.9411765336990356
Average accept_length after 23 steps : 2.17391300201416
Average accept_length after 35 steps : 2.4000000953674316
Average accept_length after 56 steps : 1.3392857313156128
Average accept_length after 10 steps : 2.0
Average accept_length after 31 steps : 2.0
Average accept_length after 15 steps : 2.4000000953674316
Average accept_length after 25 steps : 1.959999918937683
Average accept_length after 168 steps : 1.3809523582458496
Average accept_length after 82 steps : 1.8902438879013062
Average accept_length after 40 steps : 1.9500000476837158
Average accept_length after 70 steps : 1.5714285373687744
Average accept_length after 64 steps : 1.796875
Average accept_length after 53 steps : 1.9245283603668213
Average accept_length after 19 steps : 1.5263158082962036
Average accept_length after 150 steps : 1.399999976158142
Average accept_length after 51 steps : 1.7450981140136719
Average accept_length after 50 steps : 1.5799999237060547
Average accept_length after 30 steps : 2.3000001907348633
ðŸ”„ Completions saved to /home/ubuntu/sd/HandEval/outputs/medusaT0.7_medusa-vicuna-13b-v1.3/humaneval_samples.jsonl

âœ…  Results saved to /home/ubuntu/sd/HandEval/outputs/medusaT0.7_medusa-vicuna-13b-v1.3/humaneval_metrics.json
{
  "num_samples": 164,
  "samples_file": "outputs/medusaT0.7_medusa-vicuna-13b-v1.3/humaneval_samples.jsonl"
}
