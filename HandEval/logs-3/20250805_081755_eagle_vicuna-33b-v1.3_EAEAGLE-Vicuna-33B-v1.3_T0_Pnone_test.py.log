LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading EAGLE model from ../model_ckpt/EAGLE-Vicuna-33B-v1.3 with dtype float16
Base model: ../model_ckpt/vicuna-33b-v1.3
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|â–ˆâ–        | 1/7 [00:01<00:08,  1.50s/it]Loading checkpoint shards:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:03<00:07,  1.51s/it]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 3/7 [00:04<00:06,  1.51s/it]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:06<00:04,  1.51s/it]Loading checkpoint shards:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:07<00:03,  1.51s/it]Loading checkpoint shards:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:09<00:01,  1.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:09<00:00,  1.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:09<00:00,  1.42s/it]
/home/ubuntu/sd/EAGLE/eagle/model/ea_model.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ea_layer_state_dict = torch.load(load_model_path,
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/home/ubuntu/sd/EAGLE/eagle/model/cnets1.py:510: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(local_emb_path)
HumanEval Tasks:   0%|          | 0/164 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
HumanEval Tasks:   1%|          | 1/164 [00:02<05:52,  2.16s/it]HumanEval Tasks:   1%|          | 2/164 [00:03<05:17,  1.96s/it]HumanEval Tasks:   2%|â–         | 3/164 [00:05<04:14,  1.58s/it]HumanEval Tasks:   2%|â–         | 4/164 [00:05<03:28,  1.30s/it]HumanEval Tasks:   3%|â–Ž         | 5/164 [00:07<03:52,  1.47s/it]HumanEval Tasks:   4%|â–Ž         | 6/164 [00:09<04:25,  1.68s/it]HumanEval Tasks:   4%|â–         | 7/164 [00:11<04:13,  1.62s/it]HumanEval Tasks:   5%|â–         | 8/164 [00:12<03:37,  1.40s/it]HumanEval Tasks:   5%|â–Œ         | 9/164 [00:14<04:00,  1.55s/it]HumanEval Tasks:   6%|â–Œ         | 10/164 [00:16<04:23,  1.71s/it]HumanEval Tasks:   7%|â–‹         | 11/164 [00:17<04:19,  1.69s/it]HumanEval Tasks:   7%|â–‹         | 12/164 [00:18<03:51,  1.52s/it]HumanEval Tasks:   8%|â–Š         | 13/164 [00:20<04:11,  1.66s/it]HumanEval Tasks:   9%|â–Š         | 14/164 [00:21<03:38,  1.46s/it]HumanEval Tasks:   9%|â–‰         | 15/164 [00:23<03:26,  1.38s/it]HumanEval Tasks:  10%|â–‰         | 16/164 [00:23<02:50,  1.15s/it]HumanEval Tasks:  10%|â–ˆ         | 17/164 [00:24<02:39,  1.08s/it]HumanEval Tasks:  11%|â–ˆ         | 18/164 [00:25<02:40,  1.10s/it]HumanEval Tasks:  12%|â–ˆâ–        | 19/164 [00:27<02:45,  1.14s/it]HumanEval Tasks:  12%|â–ˆâ–        | 20/164 [00:28<02:50,  1.18s/it]HumanEval Tasks:  13%|â–ˆâ–Ž        | 21/164 [00:30<03:34,  1.50s/it]HumanEval Tasks:  13%|â–ˆâ–Ž        | 22/164 [00:32<03:50,  1.62s/it]HumanEval Tasks:  14%|â–ˆâ–        | 23/164 [00:33<03:31,  1.50s/it]HumanEval Tasks:  15%|â–ˆâ–        | 24/164 [00:34<02:48,  1.20s/it]HumanEval Tasks:  15%|â–ˆâ–Œ        | 25/164 [00:35<02:47,  1.21s/it]HumanEval Tasks:  16%|â–ˆâ–Œ        | 26/164 [00:37<03:30,  1.52s/it]HumanEval Tasks:  16%|â–ˆâ–‹        | 27/164 [00:38<02:53,  1.26s/it]HumanEval Tasks:  17%|â–ˆâ–‹        | 28/164 [00:39<02:31,  1.11s/it]HumanEval Tasks:  18%|â–ˆâ–Š        | 29/164 [00:39<02:15,  1.01s/it]HumanEval Tasks:  18%|â–ˆâ–Š        | 30/164 [00:40<02:11,  1.02it/s]HumanEval Tasks:  19%|â–ˆâ–‰        | 31/164 [00:41<01:43,  1.28it/s]HumanEval Tasks:  20%|â–ˆâ–‰        | 32/164 [00:41<01:38,  1.34it/s]HumanEval Tasks:  20%|â–ˆâ–ˆ        | 33/164 [00:43<02:19,  1.06s/it]HumanEval Tasks:  21%|â–ˆâ–ˆ        | 34/164 [00:44<01:55,  1.13it/s]HumanEval Tasks:  21%|â–ˆâ–ˆâ–       | 35/164 [00:44<01:53,  1.14it/s]HumanEval Tasks:  22%|â–ˆâ–ˆâ–       | 36/164 [00:46<01:59,  1.07it/s]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 37/164 [00:47<02:07,  1.01s/it]HumanEval Tasks:  23%|â–ˆâ–ˆâ–Ž       | 38/164 [00:48<02:35,  1.23s/it]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 39/164 [00:49<02:24,  1.16s/it]HumanEval Tasks:  24%|â–ˆâ–ˆâ–       | 40/164 [00:51<02:28,  1.19s/it]HumanEval Tasks:  25%|â–ˆâ–ˆâ–Œ       | 41/164 [00:52<02:17,  1.12s/it]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 42/164 [00:53<02:43,  1.34s/it]HumanEval Tasks:  26%|â–ˆâ–ˆâ–Œ       | 43/164 [00:54<02:04,  1.03s/it]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 44/164 [00:54<01:51,  1.08it/s]HumanEval Tasks:  27%|â–ˆâ–ˆâ–‹       | 45/164 [00:55<01:29,  1.32it/s]HumanEval Tasks:  28%|â–ˆâ–ˆâ–Š       | 46/164 [00:56<01:34,  1.25it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–Š       | 47/164 [00:56<01:24,  1.39it/s]HumanEval Tasks:  29%|â–ˆâ–ˆâ–‰       | 48/164 [00:58<02:02,  1.05s/it]HumanEval Tasks:  30%|â–ˆâ–ˆâ–‰       | 49/164 [00:59<02:05,  1.09s/it]HumanEval Tasks:  30%|â–ˆâ–ˆâ–ˆ       | 50/164 [01:00<01:40,  1.13it/s]HumanEval Tasks:  31%|â–ˆâ–ˆâ–ˆ       | 51/164 [01:00<01:31,  1.24it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 52/164 [01:01<01:24,  1.33it/s]HumanEval Tasks:  32%|â–ˆâ–ˆâ–ˆâ–      | 53/164 [01:01<01:11,  1.54it/s]HumanEval Tasks:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 54/164 [01:02<01:08,  1.61it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 55/164 [01:02<00:59,  1.83it/s]HumanEval Tasks:  34%|â–ˆâ–ˆâ–ˆâ–      | 56/164 [01:03<00:54,  1.99it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–      | 57/164 [01:04<01:25,  1.25it/s]HumanEval Tasks:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 58/164 [01:06<01:51,  1.05s/it]HumanEval Tasks:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 59/164 [01:08<02:13,  1.27s/it]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 60/164 [01:09<02:26,  1.41s/it]HumanEval Tasks:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 61/164 [01:10<01:57,  1.14s/it]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 62/164 [01:11<02:04,  1.22s/it]HumanEval Tasks:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 63/164 [01:12<01:55,  1.14s/it]HumanEval Tasks:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 64/164 [01:13<01:34,  1.06it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 65/164 [01:14<01:32,  1.07it/s]HumanEval Tasks:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 66/164 [01:15<01:43,  1.06s/it]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 67/164 [01:15<01:26,  1.12it/s]HumanEval Tasks:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/164 [01:16<01:27,  1.09it/s]HumanEval Tasks:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 69/164 [01:18<01:43,  1.09s/it]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 70/164 [01:19<01:30,  1.04it/s]HumanEval Tasks:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 71/164 [01:20<01:44,  1.12s/it]HumanEval Tasks:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 72/164 [01:22<01:58,  1.29s/it]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 73/164 [01:23<01:48,  1.19s/it]HumanEval Tasks:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 74/164 [01:23<01:33,  1.04s/it]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 75/164 [01:24<01:16,  1.16it/s]HumanEval Tasks:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 76/164 [01:25<01:20,  1.10it/s]HumanEval Tasks:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 77/164 [01:25<01:07,  1.28it/s]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 78/164 [01:27<01:27,  1.02s/it]HumanEval Tasks:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 79/164 [01:29<01:47,  1.26s/it]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 80/164 [01:30<01:40,  1.19s/it]HumanEval Tasks:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 81/164 [01:31<01:27,  1.06s/it]HumanEval Tasks:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 82/164 [01:32<01:38,  1.20s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 83/164 [01:33<01:24,  1.04s/it]HumanEval Tasks:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 84/164 [01:34<01:33,  1.17s/it]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 85/164 [01:35<01:23,  1.05s/it]HumanEval Tasks:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 86/164 [01:35<01:08,  1.15it/s]HumanEval Tasks:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 87/164 [01:36<01:10,  1.09it/s]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 88/164 [01:38<01:22,  1.09s/it]HumanEval Tasks:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 89/164 [01:39<01:24,  1.13s/it]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 90/164 [01:41<01:31,  1.24s/it]HumanEval Tasks:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 91/164 [01:41<01:20,  1.10s/it]HumanEval Tasks:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 92/164 [01:43<01:30,  1.25s/it]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 93/164 [01:44<01:17,  1.09s/it]HumanEval Tasks:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 94/164 [01:46<01:31,  1.31s/it]HumanEval Tasks:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 95/164 [01:47<01:40,  1.46s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 96/164 [01:48<01:29,  1.32s/it]HumanEval Tasks:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 97/164 [01:50<01:32,  1.37s/it]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 98/164 [01:52<01:53,  1.72s/it]HumanEval Tasks:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 99/164 [01:53<01:35,  1.46s/it]HumanEval Tasks:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 100/164 [01:54<01:17,  1.21s/it]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 101/164 [01:55<01:15,  1.21s/it]HumanEval Tasks:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 102/164 [01:56<01:03,  1.03s/it]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 103/164 [01:57<01:09,  1.14s/it]HumanEval Tasks:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 104/164 [01:58<00:58,  1.03it/s]HumanEval Tasks:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 105/164 [01:59<01:00,  1.03s/it]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 106/164 [02:00<01:05,  1.12s/it]HumanEval Tasks:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 107/164 [02:01<01:06,  1.17s/it]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 108/164 [02:02<01:02,  1.12s/it]HumanEval Tasks:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 109/164 [02:03<00:52,  1.04it/s]HumanEval Tasks:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 110/164 [02:05<01:00,  1.12s/it]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/164 [02:05<00:47,  1.12it/s]HumanEval Tasks:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 112/164 [02:06<00:48,  1.07it/s]HumanEval Tasks:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 113/164 [02:08<01:02,  1.22s/it]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 114/164 [02:09<01:03,  1.27s/it]HumanEval Tasks:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 115/164 [02:11<01:03,  1.29s/it]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 116/164 [02:12<01:07,  1.40s/it]HumanEval Tasks:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 117/164 [02:13<00:58,  1.25s/it]HumanEval Tasks:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 118/164 [02:14<00:46,  1.01s/it]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 119/164 [02:16<01:04,  1.44s/it]HumanEval Tasks:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 120/164 [02:17<00:56,  1.29s/it]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 121/164 [02:18<00:45,  1.07s/it]HumanEval Tasks:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 122/164 [02:18<00:38,  1.09it/s]HumanEval Tasks:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 123/164 [02:18<00:29,  1.39it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 124/164 [02:19<00:33,  1.21it/s]HumanEval Tasks:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 125/164 [02:22<00:54,  1.40s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 126/164 [02:24<00:54,  1.43s/it]HumanEval Tasks:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 127/164 [02:25<00:46,  1.26s/it]HumanEval Tasks:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 128/164 [02:27<00:56,  1.56s/it]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 129/164 [02:28<00:48,  1.39s/it]HumanEval Tasks:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 130/164 [02:31<01:06,  1.95s/it]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 131/164 [02:32<00:56,  1.70s/it]HumanEval Tasks:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 132/164 [02:33<00:43,  1.37s/it]HumanEval Tasks:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 133/164 [02:34<00:39,  1.26s/it]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 134/164 [02:34<00:30,  1.02s/it]HumanEval Tasks:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 135/164 [02:35<00:28,  1.00it/s]HumanEval Tasks:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 136/164 [02:36<00:27,  1.01it/s]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 137/164 [02:38<00:30,  1.14s/it]HumanEval Tasks:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 138/164 [02:40<00:43,  1.66s/it]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139/164 [02:43<00:44,  1.76s/it]HumanEval Tasks:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 140/164 [02:43<00:36,  1.53s/it]HumanEval Tasks:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 141/164 [02:45<00:34,  1.52s/it]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 142/164 [02:47<00:38,  1.76s/it]HumanEval Tasks:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 143/164 [02:48<00:30,  1.44s/it]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 144/164 [02:49<00:24,  1.21s/it]HumanEval Tasks:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 145/164 [02:49<00:20,  1.08s/it]HumanEval Tasks:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/164 [02:51<00:23,  1.30s/it]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 147/164 [02:52<00:18,  1.09s/it]HumanEval Tasks:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 148/164 [02:53<00:18,  1.16s/it]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 149/164 [02:55<00:19,  1.29s/it]HumanEval Tasks:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 150/164 [02:55<00:15,  1.11s/it]HumanEval Tasks:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 151/164 [02:56<00:11,  1.13it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 152/164 [02:56<00:08,  1.34it/s]HumanEval Tasks:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 153/164 [02:57<00:07,  1.52it/s]HumanEval Tasks:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 154/164 [02:58<00:09,  1.05it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 155/164 [02:59<00:08,  1.04it/s]HumanEval Tasks:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 156/164 [03:01<00:08,  1.03s/it]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 157/164 [03:03<00:10,  1.50s/it]HumanEval Tasks:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 158/164 [03:03<00:06,  1.16s/it]HumanEval Tasks:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 159/164 [03:05<00:05,  1.17s/it]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 160/164 [03:05<00:04,  1.03s/it]HumanEval Tasks:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 161/164 [03:08<00:04,  1.41s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 162/164 [03:09<00:02,  1.37s/it]HumanEval Tasks:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 163/164 [03:10<00:01,  1.35s/it]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [03:11<00:00,  1.14s/it]HumanEval Tasks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [03:11<00:00,  1.17s/it]
Average accept_length over 32 steps: 5.0312
Average accept_length over 34 steps: 3.4706
Average accept_length over 21 steps: 4.1905
Average accept_length over 16 steps: 3.8750
Average accept_length over 33 steps: 4.7879
Average accept_length over 40 steps: 3.5250
Average accept_length over 28 steps: 3.3929
Average accept_length over 17 steps: 5.3529
Average accept_length over 36 steps: 4.3056
Average accept_length over 39 steps: 4.5897
Average accept_length over 31 steps: 4.9677
Average accept_length over 21 steps: 4.4762
Average accept_length over 38 steps: 4.6053
Average accept_length over 18 steps: 4.8889
Average accept_length over 23 steps: 4.7391
Average accept_length over 11 steps: 3.7273
Average accept_length over 17 steps: 4.6471
Average accept_length over 21 steps: 4.4286
Average accept_length over 23 steps: 4.6957
Average accept_length over 24 steps: 3.8750
Average accept_length over 42 steps: 5.1429
Average accept_length over 36 steps: 5.2222
Average accept_length over 23 steps: 4.3913
Average accept_length over 9 steps: 5.2222
Average accept_length over 23 steps: 3.5217
Average accept_length over 43 steps: 4.3953
Average accept_length over 12 steps: 5.4167
Average accept_length over 14 steps: 2.5714
Average accept_length over 14 steps: 4.6429
Average accept_length over 17 steps: 5.1765
Average accept_length over 5 steps: 5.2000
Average accept_length over 12 steps: 4.4167
Average accept_length over 33 steps: 2.9697
Average accept_length over 8 steps: 3.5000
Average accept_length over 16 steps: 4.9375
Average accept_length over 20 steps: 4.0500
Average accept_length over 22 steps: 3.6818
Average accept_length over 33 steps: 4.1818
Average accept_length over 18 steps: 4.0556
Average accept_length over 24 steps: 4.9583
Average accept_length over 17 steps: 3.8235
Average accept_length over 35 steps: 4.1429
Average accept_length over 5 steps: 4.8000
Average accept_length over 12 steps: 4.6667
Average accept_length over 6 steps: 3.6667
Average accept_length over 17 steps: 4.1176
Average accept_length over 9 steps: 6.5556
Average accept_length over 35 steps: 4.0000
Average accept_length over 22 steps: 4.7727
Average accept_length over 7 steps: 3.0000
Average accept_length over 11 steps: 5.9091
Average accept_length over 11 steps: 2.2727
Average accept_length over 7 steps: 3.7143
Average accept_length over 10 steps: 5.2000
Average accept_length over 6 steps: 5.1667
Average accept_length over 7 steps: 5.5714
Average accept_length over 28 steps: 3.8214
Average accept_length over 31 steps: 4.5161
Average accept_length over 34 steps: 4.0588
Average accept_length over 33 steps: 4.2424
Average accept_length over 9 steps: 4.5556
Average accept_length over 26 steps: 4.1923
Average accept_length over 18 steps: 4.1111
Average accept_length over 8 steps: 7.0000
Average accept_length over 17 steps: 3.7647
Average accept_length over 25 steps: 4.1200
Average accept_length over 9 steps: 2.8889
Average accept_length over 17 steps: 2.6471
Average accept_length over 27 steps: 3.0000
Average accept_length over 12 steps: 2.5833
Average accept_length over 28 steps: 3.3571
Average accept_length over 32 steps: 3.0625
Average accept_length over 17 steps: 4.0000
Average accept_length over 12 steps: 3.6667
Average accept_length over 7 steps: 3.5714
Average accept_length over 19 steps: 3.4737
Average accept_length over 8 steps: 3.2500
Average accept_length over 30 steps: 2.4333
Average accept_length over 33 steps: 2.5455
Average accept_length over 19 steps: 3.0000
Average accept_length over 13 steps: 6.3846
Average accept_length over 28 steps: 4.9643
Average accept_length over 12 steps: 3.0000
Average accept_length over 28 steps: 3.2143
Average accept_length over 14 steps: 2.8571
Average accept_length over 8 steps: 3.7500
Average accept_length over 19 steps: 3.1053
Average accept_length over 27 steps: 3.2222
Average accept_length over 22 steps: 4.0455
Average accept_length over 28 steps: 2.7143
Average accept_length over 14 steps: 4.5714
Average accept_length over 30 steps: 2.6000
Average accept_length over 13 steps: 3.6154
Average accept_length over 34 steps: 3.2353
Average accept_length over 33 steps: 3.8182
Average accept_length over 18 steps: 4.2222
Average accept_length over 28 steps: 4.2143
Average accept_length over 48 steps: 4.3750
Average accept_length over 16 steps: 3.8125
Average accept_length over 11 steps: 3.7273
Average accept_length over 22 steps: 4.0909
Average accept_length over 11 steps: 1.7273
Average accept_length over 26 steps: 4.9615
Average accept_length over 10 steps: 4.6000
Average accept_length over 22 steps: 3.0909
Average accept_length over 24 steps: 4.1667
Average accept_length over 24 steps: 2.4583
Average accept_length over 18 steps: 4.7222
Average accept_length over 10 steps: 3.2000
Average accept_length over 27 steps: 3.5185
Average accept_length over 6 steps: 3.1667
Average accept_length over 19 steps: 4.3158
Average accept_length over 35 steps: 3.7714
Average accept_length over 26 steps: 2.8077
Average accept_length over 25 steps: 5.1200
Average accept_length over 30 steps: 3.8000
Average accept_length over 16 steps: 3.1875
Average accept_length over 8 steps: 4.0000
Average accept_length over 46 steps: 4.5870
Average accept_length over 17 steps: 4.2353
Average accept_length over 9 steps: 3.7778
Average accept_length over 10 steps: 3.7000
Average accept_length over 4 steps: 5.0000
Average accept_length over 19 steps: 5.5263
Average accept_length over 51 steps: 4.1373
Average accept_length over 28 steps: 3.5714
Average accept_length over 15 steps: 4.1333
Average accept_length over 42 steps: 4.1429
Average accept_length over 18 steps: 3.1667
Average accept_length over 59 steps: 3.3729
Average accept_length over 20 steps: 4.2000
Average accept_length over 11 steps: 4.3636
Average accept_length over 18 steps: 4.3889
Average accept_length over 8 steps: 3.7500
Average accept_length over 17 steps: 3.4706
Average accept_length over 18 steps: 4.1111
Average accept_length over 28 steps: 3.7500
Average accept_length over 54 steps: 3.7778
Average accept_length over 38 steps: 4.3158
Average accept_length over 18 steps: 2.8333
Average accept_length over 28 steps: 4.3214
Average accept_length over 43 steps: 5.0930
Average accept_length over 12 steps: 3.7500
Average accept_length over 12 steps: 3.6667
Average accept_length over 14 steps: 5.0000
Average accept_length over 34 steps: 3.4412
Average accept_length over 11 steps: 5.7273
Average accept_length over 24 steps: 3.9167
Average accept_length over 29 steps: 3.8276
Average accept_length over 12 steps: 3.9167
Average accept_length over 6 steps: 5.6667
Average accept_length over 7 steps: 5.0000
Average accept_length over 7 steps: 4.2857
Average accept_length over 30 steps: 3.5000
Average accept_length over 18 steps: 3.6667
Average accept_length over 22 steps: 3.7727
Average accept_length over 49 steps: 4.3878
Average accept_length over 6 steps: 4.5000
Average accept_length over 22 steps: 4.2273
Average accept_length over 12 steps: 2.5000
Average accept_length over 42 steps: 4.8333
Average accept_length over 24 steps: 2.9167
Average accept_length over 24 steps: 3.3333
Average accept_length over 12 steps: 5.8333
ðŸ”„ Completions saved to /home/ubuntu/sd/HandEval/outputs/eagleT0.0_vicuna-33b-v1.3/humaneval_samples.jsonl

âœ…  Results saved to /home/ubuntu/sd/HandEval/outputs/eagleT0.0_vicuna-33b-v1.3/humaneval_metrics.json
{
  "num_samples": 164,
  "samples_file": "outputs/eagleT0.0_vicuna-33b-v1.3/humaneval_samples.jsonl"
}
