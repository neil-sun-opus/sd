LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.50s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:07,  1.51s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.51s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.51s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:03,  1.51s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.42s/it]
Some weights of MedusaModelLlama were not initialized from the model checkpoint at /home/ubuntu/sd/model_ckpt/vicuna-33b-v1.3 and are newly initialized: ['medusa_head.0.0.linear.bias', 'medusa_head.0.0.linear.weight', 'medusa_head.0.1.weight', 'medusa_head.1.0.linear.bias', 'medusa_head.1.0.linear.weight', 'medusa_head.1.1.weight', 'medusa_head.2.0.linear.bias', 'medusa_head.2.0.linear.weight', 'medusa_head.2.1.weight', 'medusa_head.3.0.linear.bias', 'medusa_head.3.0.linear.weight', 'medusa_head.3.1.weight', 'medusa_head.4.0.linear.bias', 'medusa_head.4.0.linear.weight', 'medusa_head.4.1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  medusa_head_state_dict = torch.load(filename, map_location=model.device)
Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 0/200 [00:00<?, ?it/s]Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 1/200 [00:10<35:34, 10.72s/it]Dataset: data-is-better-together/10k_prompts_ranked:   1%|          | 2/200 [00:21<35:16, 10.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 3/200 [00:32<35:09, 10.71s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 4/200 [00:36<27:07,  8.30s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▎         | 5/200 [00:40<22:13,  6.84s/it]Dataset: data-is-better-together/10k_prompts_ranked:   3%|▎         | 6/200 [00:52<27:00,  8.35s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▎         | 7/200 [01:01<28:18,  8.80s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 8/200 [01:03<20:50,  6.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 9/200 [01:14<24:59,  7.85s/it]Dataset: data-is-better-together/10k_prompts_ranked:   5%|▌         | 10/200 [01:21<24:03,  7.60s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 11/200 [01:30<25:28,  8.09s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 12/200 [01:31<18:50,  6.01s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▋         | 13/200 [01:35<16:32,  5.31s/it]Dataset: data-is-better-together/10k_prompts_ranked:   7%|▋         | 14/200 [01:46<21:31,  6.95s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 15/200 [01:55<23:23,  7.58s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 16/200 [02:06<26:09,  8.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 17/200 [02:16<27:58,  9.17s/it]Dataset: data-is-better-together/10k_prompts_ranked:   9%|▉         | 18/200 [02:19<21:49,  7.20s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|▉         | 19/200 [02:28<23:25,  7.77s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 20/200 [02:30<17:49,  5.94s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 21/200 [02:36<17:38,  5.92s/it]Dataset: data-is-better-together/10k_prompts_ranked:  11%|█         | 22/200 [02:41<16:55,  5.70s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 23/200 [02:49<18:51,  6.39s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 24/200 [02:59<22:28,  7.66s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▎        | 25/200 [03:05<20:41,  7.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  13%|█▎        | 26/200 [03:16<23:42,  8.17s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▎        | 27/200 [03:20<19:51,  6.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 28/200 [03:30<22:58,  8.01s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 29/200 [03:40<24:39,  8.65s/it]Dataset: data-is-better-together/10k_prompts_ranked:  15%|█▌        | 30/200 [03:44<20:19,  7.17s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 31/200 [03:50<18:58,  6.73s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 32/200 [04:01<22:12,  7.93s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▋        | 33/200 [04:11<24:26,  8.78s/it]Dataset: data-is-better-together/10k_prompts_ranked:  17%|█▋        | 34/200 [04:22<25:51,  9.34s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 35/200 [04:29<23:28,  8.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 36/200 [04:39<25:07,  9.19s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 37/200 [04:50<25:48,  9.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  19%|█▉        | 38/200 [05:00<26:44,  9.90s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|█▉        | 39/200 [05:03<20:24,  7.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 40/200 [05:10<20:11,  7.57s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 41/200 [05:14<17:21,  6.55s/it]Dataset: data-is-better-together/10k_prompts_ranked:  21%|██        | 42/200 [05:25<20:41,  7.86s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 43/200 [05:30<17:52,  6.83s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 44/200 [05:40<20:43,  7.97s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▎       | 45/200 [05:48<20:30,  7.94s/it]Dataset: data-is-better-together/10k_prompts_ranked:  23%|██▎       | 46/200 [05:50<15:51,  6.18s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▎       | 47/200 [05:51<11:45,  4.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 48/200 [05:54<10:06,  3.99s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 49/200 [06:04<14:45,  5.86s/it]Dataset: data-is-better-together/10k_prompts_ranked:  25%|██▌       | 50/200 [06:15<18:14,  7.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 51/200 [06:16<13:38,  5.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 52/200 [06:26<16:58,  6.88s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▋       | 53/200 [06:30<14:26,  5.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:  27%|██▋       | 54/200 [06:33<12:30,  5.14s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 55/200 [06:34<09:03,  3.75s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 56/200 [06:36<07:56,  3.31s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 57/200 [06:46<13:07,  5.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:  29%|██▉       | 58/200 [06:47<09:12,  3.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|██▉       | 59/200 [06:54<11:52,  5.05s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 60/200 [07:05<15:43,  6.74s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 61/200 [07:06<11:39,  5.03s/it]Dataset: data-is-better-together/10k_prompts_ranked:  31%|███       | 62/200 [07:13<12:38,  5.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 63/200 [07:23<16:02,  7.02s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 64/200 [07:26<12:43,  5.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▎      | 65/200 [07:35<15:22,  6.83s/it]Dataset: data-is-better-together/10k_prompts_ranked:  33%|███▎      | 66/200 [07:41<14:20,  6.42s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▎      | 67/200 [07:43<11:38,  5.25s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 68/200 [07:49<12:08,  5.52s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 69/200 [07:50<08:57,  4.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  35%|███▌      | 70/200 [07:51<06:47,  3.14s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 71/200 [07:52<05:23,  2.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 72/200 [07:56<06:17,  2.95s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▋      | 73/200 [08:01<07:45,  3.66s/it]Dataset: data-is-better-together/10k_prompts_ranked:  37%|███▋      | 74/200 [08:11<11:19,  5.39s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 75/200 [08:22<14:34,  7.00s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 76/200 [08:32<16:44,  8.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 77/200 [08:39<15:51,  7.73s/it]Dataset: data-is-better-together/10k_prompts_ranked:  39%|███▉      | 78/200 [08:41<12:09,  5.98s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|███▉      | 79/200 [08:52<14:54,  7.40s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 80/200 [08:59<14:48,  7.40s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 81/200 [09:10<16:37,  8.38s/it]Dataset: data-is-better-together/10k_prompts_ranked:  41%|████      | 82/200 [09:12<13:00,  6.61s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 83/200 [09:13<09:29,  4.87s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 84/200 [09:18<09:10,  4.75s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▎     | 85/200 [09:23<09:46,  5.10s/it]Dataset: data-is-better-together/10k_prompts_ranked:  43%|████▎     | 86/200 [09:34<13:02,  6.87s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▎     | 87/200 [09:45<15:05,  8.01s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 88/200 [09:54<15:10,  8.13s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 89/200 [10:04<16:26,  8.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:  45%|████▌     | 90/200 [10:15<17:18,  9.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 91/200 [10:17<13:12,  7.27s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 92/200 [10:25<13:29,  7.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▋     | 93/200 [10:29<11:28,  6.43s/it]Dataset: data-is-better-together/10k_prompts_ranked:  47%|████▋     | 94/200 [10:35<11:06,  6.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 95/200 [10:37<08:28,  4.84s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 96/200 [10:42<08:53,  5.13s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 97/200 [10:46<08:03,  4.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:  49%|████▉     | 98/200 [10:57<11:01,  6.48s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|████▉     | 99/200 [11:04<11:23,  6.77s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 100/200 [11:04<07:59,  4.80s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 101/200 [11:15<10:48,  6.55s/it]Dataset: data-is-better-together/10k_prompts_ranked:  51%|█████     | 102/200 [11:22<10:45,  6.59s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 103/200 [11:28<10:47,  6.68s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [11:39<12:44,  7.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3654 > 2048). Running this sequence through the model will result in indexing errors
Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [11:39<10:46,  6.73s/it]
Average accept_length after 246 steps : 1.402438998222351
Average accept_length after 256 steps : 1.28515625
Average accept_length after 256 steps : 1.53515625
Average accept_length after 113 steps : 1.2831858396530151
Average accept_length after 103 steps : 1.524271845817566
Average accept_length after 256 steps : 1.18359375
Average accept_length after 227 steps : 1.6035242080688477
Average accept_length after 39 steps : 1.0512820482254028
Average accept_length after 256 steps : 1.93359375
Average accept_length after 171 steps : 1.5263158082962036
Average accept_length after 222 steps : 1.3648649454116821
Average accept_length after 31 steps : 0.6774193048477173
Average accept_length after 90 steps : 0.9333333373069763
Average accept_length after 256 steps : 1.69140625
Average accept_length after 206 steps : 1.713592290878296
Average accept_length after 256 steps : 1.21875
Average accept_length after 256 steps : 1.4609375
Average accept_length after 63 steps : 1.2698413133621216
Average accept_length after 171 steps : 1.1461988687515259
Average accept_length after 41 steps : 0.9756097197532654
Average accept_length after 143 steps : 1.2447552680969238
Average accept_length after 127 steps : 1.4803149700164795
Average accept_length after 194 steps : 1.2010308504104614
Average accept_length after 256 steps : 1.0859375
Average accept_length after 141 steps : 1.326241135597229
Average accept_length after 256 steps : 1.3203125
Average accept_length after 94 steps : 2.127659559249878
Average accept_length after 256 steps : 1.37109375
Average accept_length after 246 steps : 1.1626015901565552
Average accept_length after 91 steps : 1.4505494832992554
Average accept_length after 139 steps : 1.3021583557128906
Average accept_length after 256 steps : 1.15234375
Average accept_length after 256 steps : 1.40625
Average accept_length after 256 steps : 1.4765625
Average accept_length after 156 steps : 1.25
Average accept_length after 256 steps : 1.3359375
Average accept_length after 246 steps : 1.4959349632263184
Average accept_length after 256 steps : 1.46484375
Average accept_length after 55 steps : 0.3999999761581421
Average accept_length after 181 steps : 1.5027624368667603
Average accept_length after 102 steps : 1.2352941036224365
Average accept_length after 256 steps : 1.15625
Average accept_length after 109 steps : 1.0458714962005615
Average accept_length after 256 steps : 1.16796875
Average accept_length after 190 steps : 2.0684211254119873
Average accept_length after 50 steps : 2.4800000190734863
Average accept_length after 23 steps : 1.2608696222305298
Average accept_length after 62 steps : 1.2903225421905518
Average accept_length after 247 steps : 1.3117408752441406
Average accept_length after 256 steps : 1.4375
Average accept_length after 31 steps : 1.6451612710952759
Average accept_length after 240 steps : 2.320833444595337
Average accept_length after 88 steps : 1.0227272510528564
Average accept_length after 83 steps : 1.518072247505188
Average accept_length after 11 steps : 0.8181818723678589
Average accept_length after 56 steps : 1.7857143878936768
Average accept_length after 256 steps : 1.21484375
Average accept_length after 2 steps : 0.5
Average accept_length after 188 steps : 1.4255318641662598
Average accept_length after 256 steps : 1.140625
Average accept_length after 25 steps : 2.0399999618530273
Average accept_length after 161 steps : 1.1304347515106201
Average accept_length after 256 steps : 1.2265625
Average accept_length after 57 steps : 1.6315789222717285
Average accept_length after 227 steps : 1.2290748357772827
Average accept_length after 107 steps : 0.9906541705131531
Average accept_length after 59 steps : 2.271186351776123
Average accept_length after 148 steps : 2.054054021835327
Average accept_length after 19 steps : 1.3684210777282715
Average accept_length after 21 steps : 1.7142857313156128
Average accept_length after 24 steps : 1.4166667461395264
Average accept_length after 97 steps : 1.2680412530899048
Average accept_length after 130 steps : 1.2538461685180664
Average accept_length after 228 steps : 1.2850877046585083
Average accept_length after 256 steps : 1.4609375
Average accept_length after 256 steps : 1.828125
Average accept_length after 166 steps : 1.3674697875976562
Average accept_length after 46 steps : 1.2173913717269897
Average accept_length after 256 steps : 1.25
Average accept_length after 176 steps : 2.090909242630005
Average accept_length after 256 steps : 1.30078125
Average accept_length after 61 steps : 0.9180327653884888
Average accept_length after 19 steps : 1.6842105388641357
Average accept_length after 109 steps : 1.3302751779556274
Average accept_length after 144 steps : 1.3055555820465088
Average accept_length after 255 steps : 1.5803922414779663
Average accept_length after 256 steps : 1.421875
Average accept_length after 196 steps : 1.75
Average accept_length after 256 steps : 1.3671875
Average accept_length after 256 steps : 1.76171875
Average accept_length after 54 steps : 2.129629611968994
Average accept_length after 195 steps : 1.3282052278518677
Average accept_length after 96 steps : 2.0208334922790527
Average accept_length after 145 steps : 1.3793103694915771
Average accept_length after 35 steps : 1.4571428298950195
Average accept_length after 142 steps : 1.1619718074798584
Average accept_length after 90 steps : 1.2222222089767456
Average accept_length after 256 steps : 1.50390625
Average accept_length after 180 steps : 1.1277778148651123
Average accept_length after 4 steps : 1.5
Average accept_length after 256 steps : 1.453125
Average accept_length after 160 steps : 1.3625000715255737
Average accept_length after 168 steps : 1.0892857313156128
Average accept_length after 256 steps : 1.73828125
Traceback (most recent call last):
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 164, in <module>
    main(args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 148, in main
    metrics = evaluate_dataset(model, args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 106, in evaluate_dataset
    completion = model.generate(prompt, max_steps=getattr(args, "max_new_tokens", 256))
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/sd/HandEval/models/medusa_wrapper.py", line 64, in generate
    return m_generate(
  File "/home/ubuntu/sd/HandEval/models/medusa_wrapper.py", line 22, in m_generate
    for chunk in gen_iter:
  File "/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py", line 307, in medusa_generate
    medusa_logits, logits = initialize_medusa(
  File "/home/ubuntu/sd/Medusa/medusa/model/utils.py", line 146, in initialize_medusa
    medusa_logits, outputs, logits = model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/medusa_model.py", line 204, in forward
    outputs = self.base_model.model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 930, in forward
    layer_outputs = decoder_layer(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 625, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/Medusa/medusa/model/modeling_llama_kv.py", line 363, in forward
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacity of 79.10 GiB of which 899.75 MiB is free. Including non-PyTorch memory, this process has 78.21 GiB memory in use. Of the allocated memory 76.84 GiB is allocated by PyTorch, and 729.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
