LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading EAGLE model from ../model_ckpt/EAGLE-Vicuna-33B-v1.3 with dtype float16
Base model: ../model_ckpt/vicuna-33b-v1.3
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.61s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.66s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.67s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.68s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.68s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.58s/it]
/home/ubuntu/sd/EAGLE/eagle/model/ea_model.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ea_layer_state_dict = torch.load(load_model_path,
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
/home/ubuntu/sd/EAGLE/eagle/model/cnets1.py:510: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  weights = torch.load(local_emb_path)
Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 0/200 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Dataset: data-is-better-together/10k_prompts_ranked:   0%|          | 1/200 [00:04<14:17,  4.31s/it]Dataset: data-is-better-together/10k_prompts_ranked:   1%|          | 2/200 [00:08<13:15,  4.02s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 3/200 [00:11<11:31,  3.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▏         | 4/200 [00:15<12:41,  3.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:   2%|▎         | 5/200 [00:18<11:10,  3.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:   3%|▎         | 6/200 [00:21<11:26,  3.54s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▎         | 7/200 [00:25<11:02,  3.43s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 8/200 [00:26<08:44,  2.73s/it]Dataset: data-is-better-together/10k_prompts_ranked:   4%|▍         | 9/200 [00:28<08:24,  2.64s/it]Dataset: data-is-better-together/10k_prompts_ranked:   5%|▌         | 10/200 [00:31<08:33,  2.70s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 11/200 [00:35<09:23,  2.98s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▌         | 12/200 [00:36<07:26,  2.38s/it]Dataset: data-is-better-together/10k_prompts_ranked:   6%|▋         | 13/200 [00:39<08:22,  2.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:   7%|▋         | 14/200 [00:42<08:36,  2.78s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 15/200 [00:45<08:34,  2.78s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 16/200 [00:49<09:44,  3.18s/it]Dataset: data-is-better-together/10k_prompts_ranked:   8%|▊         | 17/200 [00:52<09:31,  3.12s/it]Dataset: data-is-better-together/10k_prompts_ranked:   9%|▉         | 18/200 [00:54<08:08,  2.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|▉         | 19/200 [00:58<09:14,  3.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 20/200 [00:59<07:36,  2.54s/it]Dataset: data-is-better-together/10k_prompts_ranked:  10%|█         | 21/200 [01:02<08:30,  2.85s/it]Dataset: data-is-better-together/10k_prompts_ranked:  11%|█         | 22/200 [01:05<07:57,  2.68s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 23/200 [01:09<08:51,  3.00s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▏        | 24/200 [01:12<09:30,  3.24s/it]Dataset: data-is-better-together/10k_prompts_ranked:  12%|█▎        | 25/200 [01:16<09:42,  3.33s/it]Dataset: data-is-better-together/10k_prompts_ranked:  13%|█▎        | 26/200 [01:19<09:44,  3.36s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▎        | 27/200 [01:22<09:14,  3.21s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 28/200 [01:26<09:38,  3.37s/it]Dataset: data-is-better-together/10k_prompts_ranked:  14%|█▍        | 29/200 [01:30<10:35,  3.71s/it]Dataset: data-is-better-together/10k_prompts_ranked:  15%|█▌        | 30/200 [01:33<09:54,  3.50s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 31/200 [01:37<10:22,  3.68s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▌        | 32/200 [01:41<10:28,  3.74s/it]Dataset: data-is-better-together/10k_prompts_ranked:  16%|█▋        | 33/200 [01:44<09:50,  3.54s/it]Dataset: data-is-better-together/10k_prompts_ranked:  17%|█▋        | 34/200 [01:48<09:30,  3.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 35/200 [01:52<10:04,  3.67s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 36/200 [01:56<10:32,  3.86s/it]Dataset: data-is-better-together/10k_prompts_ranked:  18%|█▊        | 37/200 [01:59<09:39,  3.55s/it]Dataset: data-is-better-together/10k_prompts_ranked:  19%|█▉        | 38/200 [02:01<08:40,  3.22s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|█▉        | 39/200 [02:05<08:54,  3.32s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 40/200 [02:08<08:51,  3.32s/it]Dataset: data-is-better-together/10k_prompts_ranked:  20%|██        | 41/200 [02:11<08:10,  3.09s/it]Dataset: data-is-better-together/10k_prompts_ranked:  21%|██        | 42/200 [02:15<08:58,  3.41s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 43/200 [02:17<08:00,  3.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▏       | 44/200 [02:22<08:56,  3.44s/it]Dataset: data-is-better-together/10k_prompts_ranked:  22%|██▎       | 45/200 [02:24<08:03,  3.12s/it]Dataset: data-is-better-together/10k_prompts_ranked:  23%|██▎       | 46/200 [02:26<06:48,  2.65s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▎       | 47/200 [02:27<05:32,  2.17s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 48/200 [02:28<05:13,  2.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  24%|██▍       | 49/200 [02:32<06:24,  2.55s/it]Dataset: data-is-better-together/10k_prompts_ranked:  25%|██▌       | 50/200 [02:36<07:03,  2.83s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 51/200 [02:39<07:38,  3.08s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▌       | 52/200 [02:42<07:01,  2.85s/it]Dataset: data-is-better-together/10k_prompts_ranked:  26%|██▋       | 53/200 [02:43<06:12,  2.53s/it]Dataset: data-is-better-together/10k_prompts_ranked:  27%|██▋       | 54/200 [02:45<05:28,  2.25s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 55/200 [02:45<04:14,  1.75s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 56/200 [02:47<03:56,  1.64s/it]Dataset: data-is-better-together/10k_prompts_ranked:  28%|██▊       | 57/200 [02:51<05:25,  2.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  29%|██▉       | 58/200 [02:51<03:50,  1.63s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|██▉       | 59/200 [02:54<05:12,  2.22s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 60/200 [02:58<06:26,  2.76s/it]Dataset: data-is-better-together/10k_prompts_ranked:  30%|███       | 61/200 [02:59<04:59,  2.16s/it]Dataset: data-is-better-together/10k_prompts_ranked:  31%|███       | 62/200 [03:04<06:40,  2.90s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 63/200 [03:08<07:15,  3.18s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▏      | 64/200 [03:09<06:10,  2.72s/it]Dataset: data-is-better-together/10k_prompts_ranked:  32%|███▎      | 65/200 [03:13<06:31,  2.90s/it]Dataset: data-is-better-together/10k_prompts_ranked:  33%|███▎      | 66/200 [03:18<08:16,  3.71s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▎      | 67/200 [03:21<07:26,  3.36s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 68/200 [03:23<06:46,  3.08s/it]Dataset: data-is-better-together/10k_prompts_ranked:  34%|███▍      | 69/200 [03:24<04:59,  2.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  35%|███▌      | 70/200 [03:24<03:48,  1.76s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 71/200 [03:25<03:00,  1.40s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▌      | 72/200 [03:28<04:09,  1.95s/it]Dataset: data-is-better-together/10k_prompts_ranked:  36%|███▋      | 73/200 [03:31<04:58,  2.35s/it]Dataset: data-is-better-together/10k_prompts_ranked:  37%|███▋      | 74/200 [03:35<05:36,  2.67s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 75/200 [03:38<06:13,  2.99s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 76/200 [03:42<06:19,  3.06s/it]Dataset: data-is-better-together/10k_prompts_ranked:  38%|███▊      | 77/200 [03:45<06:44,  3.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  39%|███▉      | 78/200 [03:47<05:44,  2.82s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|███▉      | 79/200 [03:51<06:21,  3.15s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 80/200 [03:54<06:10,  3.08s/it]Dataset: data-is-better-together/10k_prompts_ranked:  40%|████      | 81/200 [03:58<06:33,  3.31s/it]Dataset: data-is-better-together/10k_prompts_ranked:  41%|████      | 82/200 [04:00<05:40,  2.89s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 83/200 [04:02<05:15,  2.70s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▏     | 84/200 [04:06<05:49,  3.01s/it]Dataset: data-is-better-together/10k_prompts_ranked:  42%|████▎     | 85/200 [04:10<06:24,  3.34s/it]Dataset: data-is-better-together/10k_prompts_ranked:  43%|████▎     | 86/200 [04:13<06:13,  3.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▎     | 87/200 [04:17<06:26,  3.42s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 88/200 [04:20<06:08,  3.29s/it]Dataset: data-is-better-together/10k_prompts_ranked:  44%|████▍     | 89/200 [04:23<06:13,  3.37s/it]Dataset: data-is-better-together/10k_prompts_ranked:  45%|████▌     | 90/200 [04:26<05:52,  3.20s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 91/200 [04:28<05:23,  2.97s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▌     | 92/200 [04:32<05:44,  3.19s/it]Dataset: data-is-better-together/10k_prompts_ranked:  46%|████▋     | 93/200 [04:35<05:33,  3.12s/it]Dataset: data-is-better-together/10k_prompts_ranked:  47%|████▋     | 94/200 [04:39<05:45,  3.26s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 95/200 [04:40<04:28,  2.56s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 96/200 [04:42<04:29,  2.59s/it]Dataset: data-is-better-together/10k_prompts_ranked:  48%|████▊     | 97/200 [04:44<04:06,  2.39s/it]Dataset: data-is-better-together/10k_prompts_ranked:  49%|████▉     | 98/200 [04:48<04:47,  2.82s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|████▉     | 99/200 [04:52<05:22,  3.20s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 100/200 [04:52<03:48,  2.28s/it]Dataset: data-is-better-together/10k_prompts_ranked:  50%|█████     | 101/200 [04:55<04:08,  2.51s/it]Dataset: data-is-better-together/10k_prompts_ranked:  51%|█████     | 102/200 [04:59<04:31,  2.78s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 103/200 [05:05<05:57,  3.69s/it]Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [05:07<05:26,  3.40s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3654 > 2048). Running this sequence through the model will result in indexing errors
Dataset: data-is-better-together/10k_prompts_ranked:  52%|█████▏    | 104/200 [05:07<04:44,  2.96s/it]
Average accept_length over 68 steps: 2.8529
Average accept_length over 70 steps: 2.7000
Average accept_length over 53 steps: 3.8491
Average accept_length over 83 steps: 2.1084
Average accept_length over 48 steps: 3.7083
Average accept_length over 65 steps: 2.9538
Average accept_length over 57 steps: 3.5965
Average accept_length over 22 steps: 3.2727
Average accept_length over 44 steps: 4.9545
Average accept_length over 52 steps: 4.0000
Average accept_length over 66 steps: 2.9697
Average accept_length over 18 steps: 2.0000
Average accept_length over 63 steps: 1.9683
Average accept_length over 54 steps: 3.7593
Average accept_length over 48 steps: 4.4375
Average accept_length over 75 steps: 2.4267
Average accept_length over 55 steps: 3.6909
Average accept_length over 30 steps: 3.2333
Average accept_length over 54 steps: 2.9444
Average accept_length over 24 steps: 2.4167
Average accept_length over 66 steps: 2.9545
Average accept_length over 42 steps: 3.4048
Average accept_length over 69 steps: 2.7536
Average accept_length over 70 steps: 2.6857
Average accept_length over 65 steps: 2.9538
Average accept_length over 63 steps: 3.0794
Average accept_length over 52 steps: 3.9615
Average accept_length over 69 steps: 2.7391
Average accept_length over 84 steps: 2.0714
Average accept_length over 55 steps: 3.0364
Average accept_length over 76 steps: 2.4342
Average accept_length over 71 steps: 2.6197
Average accept_length over 55 steps: 3.6909
Average accept_length over 59 steps: 3.3559
Average accept_length over 75 steps: 2.4667
Average accept_length over 79 steps: 2.2785
Average accept_length over 52 steps: 4.0385
Average accept_length over 43 steps: 5.1395
Average accept_length over 66 steps: 1.7424
Average accept_length over 61 steps: 3.3279
Average accept_length over 47 steps: 2.8511
Average accept_length over 75 steps: 2.4533
Average accept_length over 41 steps: 3.4634
Average accept_length over 80 steps: 2.2500
Average accept_length over 43 steps: 5.0000
Average accept_length over 28 steps: 5.5714
Average accept_length over 19 steps: 3.0526
Average accept_length over 33 steps: 3.3636
Average accept_length over 68 steps: 2.8529
Average accept_length over 64 steps: 3.0156
Average accept_length over 67 steps: 2.8507
Average accept_length over 42 steps: 5.1429
Average accept_length over 33 steps: 2.3939
Average accept_length over 29 steps: 3.6552
Average accept_length over 10 steps: 1.4000
Average accept_length over 25 steps: 3.9600
Average accept_length over 69 steps: 2.7536
Average accept_length over 1 steps: 3.0000
Average accept_length over 66 steps: 3.0152
Average accept_length over 74 steps: 2.5135
Average accept_length over 13 steps: 4.8462
Average accept_length over 86 steps: 2.0000
Average accept_length over 71 steps: 2.6338
Average accept_length over 30 steps: 3.7000
Average accept_length over 59 steps: 3.4407
Average accept_length over 84 steps: 2.0714
Average accept_length over 44 steps: 4.8864
Average accept_length over 44 steps: 4.9773
Average accept_length over 7 steps: 3.0000
Average accept_length over 9 steps: 5.3333
Average accept_length over 9 steps: 3.1111
Average accept_length over 59 steps: 3.3220
Average accept_length over 60 steps: 3.3000
Average accept_length over 63 steps: 3.1587
Average accept_length over 68 steps: 2.7941
Average accept_length over 59 steps: 3.3559
Average accept_length over 69 steps: 2.7246
Average accept_length over 32 steps: 2.3125
Average accept_length over 71 steps: 2.6197
Average accept_length over 52 steps: 3.9808
Average accept_length over 70 steps: 2.6714
Average accept_length over 35 steps: 2.2857
Average accept_length over 41 steps: 3.3415
Average accept_length over 69 steps: 2.5362
Average accept_length over 76 steps: 2.3816
Average accept_length over 55 steps: 3.7636
Average accept_length over 69 steps: 2.7391
Average accept_length over 52 steps: 3.5577
Average accept_length over 65 steps: 2.9846
Average accept_length over 51 steps: 4.1569
Average accept_length over 44 steps: 4.6136
Average accept_length over 68 steps: 2.6029
Average accept_length over 54 steps: 3.7778
Average accept_length over 66 steps: 2.9394
Average accept_length over 16 steps: 4.8125
Average accept_length over 49 steps: 2.1633
Average accept_length over 35 steps: 3.5429
Average accept_length over 70 steps: 2.7857
Average accept_length over 75 steps: 2.4267
Average accept_length over 2 steps: 4.5000
Average accept_length over 55 steps: 3.6909
Average accept_length over 61 steps: 3.2787
Average accept_length over 108 steps: 1.3796
Average accept_length over 48 steps: 4.4375
Traceback (most recent call last):
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 164, in <module>
    main(args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 148, in main
    metrics = evaluate_dataset(model, args)
  File "/home/ubuntu/sd/HandEval/test_p2.py", line 106, in evaluate_dataset
    completion = model.generate(prompt, max_steps=getattr(args, "max_new_tokens", 256))
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/sd/HandEval/models/eagle_wrapper.py", line 85, in generate
    output_ids = self.model.eagenerate(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ubuntu/sd/EAGLE/eagle/model/ea_model.py", line 240, in eagenerate
    draft_tokens, retrieve_indices, tree_mask, tree_position_ids, logits, hidden_state, sample_token = initialize_tree(
  File "/home/ubuntu/sd/EAGLE/eagle/model/utils.py", line 233, in initialize_tree
    outputs, orig, hidden_states = model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/EAGLE/eagle/model/ea_model.py", line 178, in forward
    outputs = self.base_model.model(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/EAGLE/eagle/model/modeling_llama_kv.py", line 1162, in forward
    layer_outputs = decoder_layer(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/EAGLE/eagle/model/modeling_llama_kv.py", line 839, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/sd/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu/sd/EAGLE/eagle/model/modeling_llama_kv.py", line 737, in forward
    attn_weights = attn_weights + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.59 GiB. GPU 0 has a total capacity of 79.10 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 77.56 GiB memory in use. Of the allocated memory 76.05 GiB is allocated by PyTorch, and 873.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
